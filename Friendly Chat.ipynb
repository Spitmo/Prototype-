{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba90272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friendly Dataset Dialogue Act Classification using BERT\n",
    "# Complete training pipeline for chatbot dialogue act recognition\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FRIENDLY DATASET DIALOGUE ACT CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Load dataset from CSV files\n",
    "print(\"\\n1. Loading Dataset...\")\n",
    "dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\train.csv\",\n",
    "    \"validation\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\validation.csv\",\n",
    "    \"test\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\test.csv\"\n",
    "})\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Explore dataset structure\n",
    "print(\"\\nDataset structure:\")\n",
    "print(\"Train columns:\", dataset['train'].column_names)\n",
    "print(\"Sample data:\")\n",
    "for i in range(3):\n",
    "    sample = dataset['train'][i]\n",
    "    print(f\"Dialog: {sample['dialog']}\")\n",
    "    print(f\"Act: {sample['act']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 2. Preprocess dialog data\n",
    "print(\"\\n2. Preprocessing Dialog Data...\")\n",
    "def preprocess(batch):\n",
    "    # Join list of utterances if necessary\n",
    "    if isinstance(batch[\"dialog\"][0], list):\n",
    "        batch[\"dialog\"] = [\" \".join(conv) for conv in batch[\"dialog\"]]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "print(\"Dialog preprocessing completed!\")\n",
    "\n",
    "# 3. Label encoding - FIXED VERSION\n",
    "print(\"\\n3. Label Encoding...\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create label encoder and fit on ALL data splits\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Collect all acts from all splits\n",
    "all_acts = []\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    all_acts.extend(dataset[split]['act'])\n",
    "\n",
    "# Fit label encoder on all unique acts\n",
    "label_encoder.fit(all_acts)\n",
    "\n",
    "print(f\"Unique dialogue acts found: {len(label_encoder.classes_)}\")\n",
    "print(\"Label mapping:\")\n",
    "for i, act in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {act}\")\n",
    "\n",
    "# Apply label encoding to each split\n",
    "def encode_labels(example):\n",
    "    return {\"act_label\": label_encoder.transform([example[\"act\"]])[0]}\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "print(\"Label encoding completed!\")\n",
    "\n",
    "# 4. Initialize tokenizer from local path\n",
    "print(\"\\n4. Loading Tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"dialog\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "print(\"Tokenization completed!\")\n",
    "\n",
    "# 5. Set format and create datasets\n",
    "print(\"\\n5. Creating PyTorch Datasets...\")\n",
    "dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"act_label\"]\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# 6. Custom collate function - FIXED VERSION\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    # Ensure labels are long integers and use correct key name\n",
    "    labels = torch.tensor([int(item[\"act_label\"]) for item in batch], dtype=torch.long)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels  # Changed from \"act_label\" to \"labels\"\n",
    "    }\n",
    "\n",
    "# 7. Create DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {batch_size}\")\n",
    "\n",
    "# 8. Initialize model from local path\n",
    "print(\"\\n6. Loading BERT Model...\")\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "print(f\"Model loaded with {num_labels} output classes\")\n",
    "\n",
    "# 9. Setup optimizer and scheduler\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Training setup - Epochs: {num_epochs}, Learning rate: {learning_rate}\")\n",
    "\n",
    "# 10. Training functions\n",
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)  # Using correct key \"labels\"\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)  # Using correct key \"labels\"\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n",
    "\n",
    "# 11. Training loop\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_state = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_accuracy, val_predictions, val_labels = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"✅ New best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# 12. Load best model and final evaluation\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n📥 Loaded best model with validation accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_loss, test_accuracy, test_predictions, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 13. Detailed classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "target_names = label_encoder.classes_\n",
    "print(classification_report(test_labels, test_predictions, target_names=target_names))\n",
    "\n",
    "# 14. Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix - Dialogue Act Classification')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 15. Training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "class_counts = pd.Series(test_labels).value_counts().sort_index()\n",
    "plt.bar(range(len(class_counts)), class_counts.values)\n",
    "plt.title('Test Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(len(target_names)), target_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 16. Save the trained model\n",
    "model_save_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_friendly_bert\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save label encoder\n",
    "with open(model_save_path + \"/label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save training history\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'best_accuracy': best_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'label_classes': label_encoder.classes_.tolist()\n",
    "}\n",
    "\n",
    "with open(model_save_path + \"/training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_history, f)\n",
    "\n",
    "print(f\"\\n✅ Model saved to: {model_save_path}\")\n",
    "print(f\"📊 Training history saved\")\n",
    "print(f\"🏷️ Label encoder saved\")\n",
    "\n",
    "# 17. Inference function for dialogue act prediction\n",
    "def predict_dialogue_act(text, model, tokenizer, label_encoder, device, max_length=64):\n",
    "    \"\"\"\n",
    "    Predict dialogue act for a given text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    # Decode prediction\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return {\n",
    "        'predicted_act': predicted_label,\n",
    "        'confidence': confidence,\n",
    "        'all_probabilities': {act: prob.item() for act, prob in zip(label_encoder.classes_, predictions[0])}\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"🎯 Best Validation Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"🧪 Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"💾 Model saved for inference\")\n",
    "\n",
    "# Example usage of the inference function\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"EXAMPLE DIALOGUE ACT PREDICTIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "example_texts = [\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"Thank you so much for your help!\",\n",
    "    \"Can you please help me with this problem?\",\n",
    "    \"I don't understand what you mean\",\n",
    "    \"That sounds like a great idea!\",\n",
    "    \"Sorry, I made a mistake there\",\n",
    "    \"What time is it right now?\",\n",
    "    \"Have a wonderful day!\"\n",
    "]\n",
    "\n",
    "for text in example_texts:\n",
    "    result = predict_dialogue_act(text, model, tokenizer, label_encoder, device)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Dialogue Act: {result['predicted_act']} (Confidence: {result['confidence']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIALOGUE ACT CLASSIFIER READY FOR CHATBOT INTEGRATION!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
