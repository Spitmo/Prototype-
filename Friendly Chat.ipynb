{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba90272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FAST TRAINING MODE - OPTIMIZED FOR DEMO\n",
      "============================================================\n",
      "‚ö° FAST TRAINING MODE FOR DEMO\n",
      "‚è±Ô∏è  Estimated total time: 30-60 minutes\n",
      "üéØ Optimizations: Small dataset, 1 epoch, larger batches, shorter sequences\n",
      "\n",
      "============================================================\n",
      "\n",
      "üß† TRAINING MENTAL HEALTH MODEL (FAST MODE)\n",
      "--------------------------------------------------\n",
      "Using device: cpu\n",
      "üìÇ Loading dataset...\n",
      "Original dataset: 53043 samples\n",
      "Using for training: 5000 samples (FAST MODE)\n",
      "After cleaning: 4959 samples\n",
      "Class distribution:\n",
      "status\n",
      "Normal                  1565\n",
      "Depression              1505\n",
      "Suicidal                 902\n",
      "Anxiety                  351\n",
      "Stress                   272\n",
      "Bipolar                  261\n",
      "Personality disorder     103\n",
      "Name: count, dtype: int64\n",
      "Train: 3967, Test: 992\n",
      "üìö Loading tokenizer...\n",
      "‚úÖ Using local BERT tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÉ‚Äç‚ôÇÔ∏è FAST TRAINING: 1 epoch, LR: 5e-05, Batch: 32\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [08:27<00:00,  4.09s/it, loss=1.1460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:30<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6310\n",
      "‚úÖ Mental Health Model saved to: C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_mental_health_bert_fast\n",
      "üéØ Final Accuracy: 0.6310\n",
      "‚è±Ô∏è  Mental Health Model trained in: 9.2 minutes\n",
      "\n",
      "üí¨ TRAINING DIALOGUE ACT MODEL (FAST MODE)\n",
      "--------------------------------------------------\n",
      "üìÇ Loading dialogue dataset...\n",
      "Sampled - Train: 1667, Val: 500, Test: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:00<00:00, 15118.38 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 10030.04 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 11302.60 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:04<00:00, 338.35 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 360.41 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 360.69 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:03<00:00, 510.55 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 488.57 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 478.75 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting dialogue act training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Dialogue Acts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [03:39<00:00,  4.14s/it, loss=7.1889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:15<00:00,  1.04it/s]\n",
      "c:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0060\n",
      "‚úÖ Dialogue Act Model saved to: C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_fast\n",
      "üéØ Final Accuracy: 0.0060\n",
      "‚è±Ô∏è  Dialogue Act Model trained in: 4.2 minutes\n",
      "\n",
      "============================================================\n",
      "üéâ FAST TRAINING COMPLETED!\n",
      "‚è±Ô∏è  Total Training Time: 13.5 minutes\n",
      "‚úÖ Both models ready for demo!\n",
      "============================================================\n",
      "\n",
      "üß™ QUICK DEMO TEST:\n",
      "‚ùå Error during training: name 'device' is not defined\n",
      "\n",
      "üìù Creating demo functions...\n",
      "‚úÖ Demo file created: demo_chatbot.py\n",
      "üéØ Ready for tomorrow's presentation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\NAMAN\\AppData\\Local\\Temp\\ipykernel_19872\\551727435.py\", line 432, in <module>\n",
      "    mental_pred, mental_conf = quick_mental_health_test(msg)\n",
      "                               ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\NAMAN\\AppData\\Local\\Temp\\ipykernel_19872\\551727435.py\", line 394, in quick_mental_health_test\n",
      "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
      "                      ^^^^^^\n",
      "NameError: name 'device' is not defined\n"
     ]
    }
   ],
   "source": [
    "# FAST TRAINING OPTIMIZATION FOR DEMO\n",
    "# Optimized versions for 30-60 minute training time\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "print(\"üöÄ FAST TRAINING MODE - OPTIMIZED FOR DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# FAST MENTAL HEALTH MODEL (15-20 minutes)\n",
    "# ============================================================================\n",
    "\n",
    "def train_mental_health_fast():\n",
    "    \"\"\"Fast training for mental health model\"\"\"\n",
    "    print(\"\\nüß† TRAINING MENTAL HEALTH MODEL (FAST MODE)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Load and sample dataset (MUCH SMALLER)\n",
    "    print(\"üìÇ Loading dataset...\")\n",
    "    data_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Combined Data.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # OPTIMIZATION 1: Use only 10% of data for demo\n",
    "    sample_size = min(5000, len(df))  # Max 5000 samples\n",
    "    df_sampled = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original dataset: {len(df)} samples\")\n",
    "    print(f\"Using for training: {len(df_sampled)} samples (FAST MODE)\")\n",
    "    \n",
    "    # Basic cleaning\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        return str(text).lower().strip()\n",
    "    \n",
    "    df_sampled['statement'] = df_sampled['statement'].apply(clean_text)\n",
    "    df_sampled = df_sampled[df_sampled['statement'].str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After cleaning: {len(df_sampled)} samples\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(df_sampled['status'].value_counts())\n",
    "    \n",
    "    # 2. Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_sampled['encoded_label'] = label_encoder.fit_transform(df_sampled['status'])\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    \n",
    "    # 3. OPTIMIZATION 2: Smaller train/val split (80/20 instead of 70/15/15)\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        df_sampled['statement'].tolist(),\n",
    "        df_sampled['encoded_label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df_sampled['encoded_label']\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_texts)}, Test: {len(test_texts)}\")\n",
    "    \n",
    "    # 4. Load tokenizer (try local first, fallback to online)\n",
    "    print(\"üìö Loading tokenizer...\")\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\")\n",
    "        print(\"‚úÖ Using local BERT tokenizer\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Local BERT not found, using online version...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        print(\"‚úÖ Using online BERT tokenizer\")\n",
    "    \n",
    "    # 5. OPTIMIZATION 3: Smaller max_length for faster processing\n",
    "    def tokenize_texts(texts, labels, max_length=32):  # Reduced from 64 to 32\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    train_encodings = tokenize_texts(train_texts, train_labels)\n",
    "    test_encodings = tokenize_texts(test_texts, test_labels)\n",
    "    \n",
    "    # 6. Dataset class\n",
    "    class FastDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {k: v[idx] for k, v in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings['labels'])\n",
    "    \n",
    "    train_dataset = FastDataset(train_encodings)\n",
    "    test_dataset = FastDataset(test_encodings)\n",
    "    \n",
    "    # 7. OPTIMIZATION 4: Larger batch size for faster training\n",
    "    batch_size = 32  # Increased from 16 to 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # 8. Load model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\",\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # 9. OPTIMIZATION 5: Higher learning rate + fewer epochs\n",
    "    num_epochs = 1  # Just 1 epoch for demo\n",
    "    learning_rate = 5e-5  # Higher learning rate\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    print(f\"\\nüèÉ‚Äç‚ôÇÔ∏è FAST TRAINING: {num_epochs} epoch, LR: {learning_rate}, Batch: {batch_size}\")\n",
    "    \n",
    "    # 10. Training function\n",
    "    def train_epoch_fast(model, train_loader, optimizer, lr_scheduler, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def evaluate_fast(model, test_loader, device):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        return accuracy, all_predictions, all_labels\n",
    "    \n",
    "    # 11. Train\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    train_loss = train_epoch_fast(model, train_loader, optimizer, lr_scheduler, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # 12. Evaluate\n",
    "    test_accuracy, test_predictions, test_labels = evaluate_fast(model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # 13. Save model\n",
    "    model_save_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_mental_health_bert_fast\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    with open(f\"{model_save_path}/label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"‚úÖ Mental Health Model saved to: {model_save_path}\")\n",
    "    print(f\"üéØ Final Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "# ============================================================================\n",
    "# FAST DIALOGUE ACT MODEL (15-20 minutes)\n",
    "# ============================================================================\n",
    "\n",
    "def train_dialogue_act_fast():\n",
    "    \"\"\"Fast training for dialogue act model\"\"\"\n",
    "    print(\"\\nüí¨ TRAINING DIALOGUE ACT MODEL (FAST MODE)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 1. Load dataset with sampling\n",
    "    print(\"üìÇ Loading dialogue dataset...\")\n",
    "    dataset = load_dataset(\"csv\", data_files={\n",
    "        \"train\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\train.csv\",\n",
    "        \"validation\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\validation.csv\",\n",
    "        \"test\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\test.csv\"\n",
    "    })\n",
    "    \n",
    "    # OPTIMIZATION 1: Sample smaller subset\n",
    "    def sample_dataset(dataset_split, sample_ratio=0.1):  # Use only 10% of data\n",
    "        total_samples = len(dataset_split)\n",
    "        sample_size = max(500, int(total_samples * sample_ratio))  # At least 500 samples\n",
    "        indices = random.sample(range(total_samples), min(sample_size, total_samples))\n",
    "        return dataset_split.select(indices)\n",
    "    \n",
    "    dataset['train'] = sample_dataset(dataset['train'], 0.15)  # 15% for training\n",
    "    dataset['validation'] = sample_dataset(dataset['validation'], 0.3)  # 30% for validation\n",
    "    dataset['test'] = sample_dataset(dataset['test'], 0.3)  # 30% for test\n",
    "    \n",
    "    print(f\"Sampled - Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")\n",
    "    \n",
    "    # 2. Preprocess\n",
    "    def preprocess(batch):\n",
    "        if isinstance(batch[\"dialog\"][0], list):\n",
    "            batch[\"dialog\"] = [\" \".join(conv) for conv in batch[\"dialog\"]]\n",
    "        return batch\n",
    "    \n",
    "    dataset = dataset.map(preprocess, batched=True)\n",
    "    \n",
    "    # 3. Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_acts = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_acts.extend(dataset[split]['act'])\n",
    "    label_encoder.fit(all_acts)\n",
    "    \n",
    "    def encode_labels(example):\n",
    "        return {\"act_label\": label_encoder.transform([example[\"act\"]])[0]}\n",
    "    \n",
    "    dataset = dataset.map(encode_labels)\n",
    "    \n",
    "    # 4. Tokenization\n",
    "    tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\")\n",
    "    \n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"dialog\"], truncation=True, padding=\"max_length\", max_length=32)  # Reduced to 32\n",
    "    \n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"act_label\"])\n",
    "    \n",
    "    # 5. DataLoaders\n",
    "    def collate_fn(batch):\n",
    "        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "        attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "        labels = torch.tensor([int(item[\"act_label\"]) for item in batch], dtype=torch.long)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    batch_size = 32  # Larger batch size\n",
    "    train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(dataset[\"validation\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(dataset[\"test\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # 6. Model\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\",\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # 7. Training setup\n",
    "    num_epochs = 1  # Just 1 epoch\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    # 8. Train (same functions as above)\n",
    "    print(\"üöÄ Starting dialogue act training...\")\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training Dialogue Acts\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # 9. Evaluate\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # 10. Save\n",
    "    model_save_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_fast\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    with open(f\"{model_save_path}/label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"‚úÖ Dialogue Act Model saved to: {model_save_path}\")\n",
    "    print(f\"üéØ Final Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    print(\"‚ö° FAST TRAINING MODE FOR DEMO\")\n",
    "    print(\"‚è±Ô∏è  Estimated total time: 30-60 minutes\")\n",
    "    print(\"üéØ Optimizations: Small dataset, 1 epoch, larger batches, shorter sequences\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Train both models\n",
    "    try:\n",
    "        # Mental Health Model (15-20 mins)\n",
    "        start_time = time.time()\n",
    "        mental_health_model, mental_health_tokenizer, mental_health_encoder = train_mental_health_fast()\n",
    "        mental_health_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è  Mental Health Model trained in: {mental_health_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Dialogue Act Model (15-20 mins)\n",
    "        start_time = time.time()\n",
    "        dialogue_model, dialogue_tokenizer, dialogue_encoder = train_dialogue_act_fast()\n",
    "        dialogue_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è  Dialogue Act Model trained in: {dialogue_time/60:.1f} minutes\")\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéâ FAST TRAINING COMPLETED!\")\n",
    "        print(f\"‚è±Ô∏è  Total Training Time: {total_time/60:.1f} minutes\")\n",
    "        print(\"‚úÖ Both models ready for demo!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Quick test\n",
    "        print(\"\\nüß™ QUICK DEMO TEST:\")\n",
    "        \n",
    "        # Test mental health\n",
    "        def quick_mental_health_test(text):\n",
    "            model = mental_health_model\n",
    "            tokenizer = mental_health_tokenizer\n",
    "            encoder = mental_health_encoder\n",
    "            \n",
    "            inputs = tokenizer(text, truncation=True, padding='max_length', max_length=32, return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "                confidence = predictions[0][predicted_class].item()\n",
    "            \n",
    "            predicted_label = encoder.inverse_transform([predicted_class])[0]\n",
    "            return predicted_label, confidence\n",
    "        \n",
    "        # Test dialogue act\n",
    "        def quick_dialogue_test(text):\n",
    "            model = dialogue_model\n",
    "            tokenizer = dialogue_tokenizer\n",
    "            encoder = dialogue_encoder\n",
    "            \n",
    "            inputs = tokenizer(text, truncation=True, padding='max_length', max_length=32, return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "                confidence = predictions[0][predicted_class].item()\n",
    "            \n",
    "            predicted_label = encoder.inverse_transform([predicted_class])[0]\n",
    "            return predicted_label, confidence\n",
    "        \n",
    "        # Demo tests\n",
    "        test_messages = [\n",
    "            \"Hello, how are you today?\",\n",
    "            \"I feel so anxious and stressed\",\n",
    "            \"Thank you for your help!\",\n",
    "            \"I can't handle this anymore\"\n",
    "        ]\n",
    "        \n",
    "        for msg in test_messages:\n",
    "            mental_pred, mental_conf = quick_mental_health_test(msg)\n",
    "            dialogue_pred, dialogue_conf = quick_dialogue_test(msg)\n",
    "            print(f\"\\nMessage: '{msg}'\")\n",
    "            print(f\"  Mental Health: {mental_pred} ({mental_conf:.3f})\")\n",
    "            print(f\"  Dialogue Act: {dialogue_pred} ({dialogue_conf:.3f})\")\n",
    "        \n",
    "        print(\"\\nüöÄ MODELS ARE READY FOR YOUR DEMO TOMORROW! üöÄ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Quick demo inference functions for tomorrow\n",
    "def create_demo_functions():\n",
    "    \"\"\"\n",
    "    Create simple demo functions for tomorrow's presentation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìù Creating demo functions...\")\n",
    "    \n",
    "    demo_code = '''# DEMO FUNCTIONS FOR TOMORROW'S PRESENTATION\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class DemoChatbot:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        # Load Mental Health Model\n",
    "        self.mental_health_model = BertForSequenceClassification.from_pretrained(\n",
    "            r\"C:\\\\Users\\\\NAMAN\\\\Documents\\\\GitHub\\\\Prototype-\\\\trained_mental_health_bert_fast\"\n",
    "        )\n",
    "        self.mental_health_tokenizer = BertTokenizer.from_pretrained(\n",
    "            r\"C:\\\\Users\\\\NAMAN\\\\Documents\\\\GitHub\\\\Prototype-\\\\trained_mental_health_bert_fast\"\n",
    "        )\n",
    "        with open(r\"C:\\\\Users\\\\NAMAN\\\\Documents\\\\GitHub\\\\Prototype-\\\\trained_mental_health_bert_fast\\\\label_encoder.pkl\", \"rb\") as f:\n",
    "            self.mental_health_encoder = pickle.load(f)\n",
    "        \n",
    "        # Load Dialogue Act Model\n",
    "        self.dialogue_model = BertForSequenceClassification.from_pretrained(\n",
    "            r\"C:\\\\Users\\\\NAMAN\\\\Documents\\\\GitHub\\\\Prototype-\\\\trained_dialogue_bert_fast\"\n",
    "        )\n",
    "        self.dialogue_tokenizer = BertTokenizer.from_pretrained(\n",
    "            r\"C:\\\\Users\\\\NAMAN\\\\Documents\\\\GitHub\\\\Prototype-\\\\trained_dialogue_bert_fast\"\n",
    "        )\n",
    "        with open(r\"C:\\\\Users\\\\NAMAN\\\\Documents\\\\GitHub\\\\Prototype-\\\\trained_dialogue_bert_fast\\\\label_encoder.pkl\", \"rb\") as f:\n",
    "            self.dialogue_encoder = pickle.load(f)\n",
    "        \n",
    "        self.mental_health_model.to(self.device).eval()\n",
    "        self.dialogue_model.to(self.device).eval()\n",
    "    \n",
    "    def analyze_message(self, text):\n",
    "        \"\"\"Complete analysis for demo\"\"\"\n",
    "        \n",
    "        # Mental Health Analysis\n",
    "        inputs = self.mental_health_tokenizer(text, truncation=True, padding='max_length', \n",
    "                                            max_length=32, return_tensors='pt')\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.mental_health_model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            mental_class = torch.argmax(predictions, dim=-1).item()\n",
    "            mental_conf = predictions[0][mental_class].item()\n",
    "        \n",
    "        mental_state = self.mental_health_encoder.inverse_transform([mental_class])[0]\n",
    "        \n",
    "        # Dialogue Act Analysis\n",
    "        inputs = self.dialogue_tokenizer(text, truncation=True, padding='max_length', \n",
    "                                       max_length=32, return_tensors='pt')\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.dialogue_model(**outputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            dialogue_class = torch.argmax(predictions, dim=-1).item()\n",
    "            dialogue_conf = predictions[0][dialogue_class].item()\n",
    "        \n",
    "        dialogue_act = self.dialogue_encoder.inverse_transform([dialogue_class])[0]\n",
    "        \n",
    "        return {\n",
    "            'message': text,\n",
    "            'mental_health': {\n",
    "                'state': mental_state,\n",
    "                'confidence': mental_conf,\n",
    "                'needs_attention': mental_state in ['Anxiety', 'Depression', 'Suicidal', 'Stress']\n",
    "            },\n",
    "            'dialogue_act': {\n",
    "                'act': dialogue_act,\n",
    "                'confidence': dialogue_conf\n",
    "            }\n",
    "        }\n",
    "\n",
    "# DEMO USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = DemoChatbot()\n",
    "    \n",
    "    demo_messages = [\n",
    "        \"Hello! How can you help me?\",\n",
    "        \"I've been feeling really anxious lately\",\n",
    "        \"Thank you so much for listening\",\n",
    "        \"I don't know what to do anymore, everything seems hopeless\",\n",
    "        \"Can you recommend some coping strategies?\",\n",
    "        \"That was very helpful, I appreciate it\"\n",
    "    ]\n",
    "    \n",
    "    print(\"CHATBOT DEMO - LIVE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for msg in demo_messages:\n",
    "        result = chatbot.analyze_message(msg)\n",
    "        print(f\"\\\\nUser: {msg}\")\n",
    "        print(f\"Mental State: {result['mental_health']['state']} ({result['mental_health']['confidence']:.2f})\")\n",
    "        print(f\"Dialogue Act: {result['dialogue_act']['act']} ({result['dialogue_act']['confidence']:.2f})\")\n",
    "        if result['mental_health']['needs_attention']:\n",
    "            print(\"WARNING: Mental health attention needed!\")\n",
    "'''\n",
    "    \n",
    "    with open(\"demo_chatbot.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(demo_code)\n",
    "    \n",
    "    print(\"‚úÖ Demo file created: demo_chatbot.py\")\n",
    "    print(\"üéØ Ready for tomorrow's presentation!\")\n",
    "\n",
    "# Run the demo file creation\n",
    "create_demo_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0da8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FINAL IMPROVED TRAINING - TARGET 75-80% ACCURACY\n",
      "============================================================\n",
      "üéØ TONIGHT'S IMPROVED TRAINING SESSION\n",
      "Target: 75-80% accuracy in ~1 hour\n",
      "============================================================\n",
      "Phase 1: Mental Health Model (Priority)\n",
      "\n",
      "üß† IMPROVED MENTAL HEALTH MODEL\n",
      "--------------------------------------------------\n",
      "Using device: cpu\n",
      "üìÇ Loading dataset...\n",
      "Using 25000 samples (was 5,000)\n",
      "After cleaning: 24774 samples\n",
      "Original class distribution:\n",
      "status\n",
      "Normal                  7718\n",
      "Depression              7287\n",
      "Suicidal                4962\n",
      "Anxiety                 1774\n",
      "Bipolar                 1287\n",
      "Stress                  1239\n",
      "Personality disorder     507\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚öñÔ∏è Balancing dataset...\n",
      "  Depression: 7287 ‚Üí 3000 (undersampled)\n",
      "  Normal: 7718 ‚Üí 3000 (undersampled)\n",
      "  Suicidal: 4962 ‚Üí 3000 (undersampled)\n",
      "  Anxiety: 1774 (unchanged)\n",
      "  Bipolar: 1287 (unchanged)\n",
      "  Stress: 1239 (unchanged)\n",
      "  Personality disorder: 507 ‚Üí 1000 (oversampled)\n",
      "\n",
      "Balanced dataset: 14300 samples\n",
      "Balanced class distribution:\n",
      "status\n",
      "Normal                  3000\n",
      "Depression              3000\n",
      "Suicidal                3000\n",
      "Anxiety                 1774\n",
      "Bipolar                 1287\n",
      "Stress                  1239\n",
      "Personality disorder    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping (7 classes):\n",
      "  0: Anxiety\n",
      "  1: Bipolar\n",
      "  2: Depression\n",
      "  3: Normal\n",
      "  4: Personality disorder\n",
      "  5: Stress\n",
      "  6: Suicidal\n",
      "\n",
      "Dataset splits:\n",
      "  Train: 11440 samples\n",
      "  Validation: 2860 samples\n",
      "\n",
      "üìö Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using local BERT model\n",
      "üî§ Tokenizing texts...\n",
      "üì¶ Created DataLoaders with batch size: 16\n",
      "\n",
      "üîß Training configuration:\n",
      "  Epochs: 3\n",
      "  Learning rate: 2e-05\n",
      "  Batch size: 16\n",
      "  Max length: 64 tokens\n",
      "  Weight decay: 0.01\n",
      "  Warmup ratio: 0.1\n",
      "\n",
      "‚öñÔ∏è Class weights computed:\n",
      "  Anxiety: 1.152\n",
      "  Bipolar: 1.587\n",
      "  Depression: 0.681\n",
      "  Normal: 0.681\n",
      "  Personality disorder: 2.043\n",
      "  Stress: 1.649\n",
      "  Suicidal: 0.681\n",
      "üìà Scheduler: cosine with 214 warmup steps\n",
      "\n",
      "üöÄ Starting improved training...\n",
      "\n",
      "Epoch 1/3\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 715/715 [51:44<00:00,  4.34s/it, loss=0.4550]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [02:26<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1136\n",
      "Val Loss: 0.6399\n",
      "Val Accuracy: 0.7493\n",
      "Val F1-Score: 0.7420\n",
      "Epoch Time: 54.2 minutes\n",
      "‚úÖ New best model! F1: 0.7420, Accuracy: 0.7493\n",
      "\n",
      "Epoch 2/3\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 715/715 [52:57<00:00,  4.44s/it, loss=0.1573]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [04:46<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4903\n",
      "Val Loss: 0.5613\n",
      "Val Accuracy: 0.7902\n",
      "Val F1-Score: 0.7887\n",
      "Epoch Time: 57.7 minutes\n",
      "‚úÖ New best model! F1: 0.7887, Accuracy: 0.7902\n",
      "\n",
      "Epoch 3/3\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 715/715 [50:33<00:00,  4.24s/it, loss=0.7164]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [02:52<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3079\n",
      "Val Loss: 0.5727\n",
      "Val Accuracy: 0.7913\n",
      "Val F1-Score: 0.7892\n",
      "Epoch Time: 53.4 minutes\n",
      "‚úÖ New best model! F1: 0.7892, Accuracy: 0.7913\n",
      "\n",
      "üì• Loaded best model with F1: 0.7892, Accuracy: 0.7913\n",
      "\n",
      "üìä Final Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179/179 [02:41<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Results:\n",
      "  Accuracy: 0.7913\n",
      "  F1-Score: 0.7892\n",
      "  Training Time: 165.3 minutes\n",
      "\n",
      "üìã Detailed Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety     0.8736    0.8563    0.8649       355\n",
      "             Bipolar     0.7950    0.8599    0.8262       257\n",
      "          Depression     0.7440    0.5717    0.6466       600\n",
      "              Normal     0.9308    0.8967    0.9134       600\n",
      "Personality disorder     0.8318    0.8900    0.8599       200\n",
      "              Stress     0.7174    0.7984    0.7557       248\n",
      "            Suicidal     0.6823    0.8017    0.7372       600\n",
      "\n",
      "            accuracy                         0.7913      2860\n",
      "           macro avg     0.7964    0.8107    0.8005      2860\n",
      "        weighted avg     0.7947    0.7913    0.7892      2860\n",
      "\n",
      "\n",
      "‚úÖ Improved model saved to: C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_mental_health_bert_improved\n",
      "üìà Improvement: 63.1% ‚Üí 79.1%\n",
      "\n",
      "üìä Mental Health Results:\n",
      "  Accuracy: 79.1%\n",
      "  Training time: 168.7 minutes\n",
      "  Improvement: 63.1% ‚Üí 79.1%\n",
      "\n",
      "Phase 2: Dialogue Act Model\n",
      "\n",
      "üí¨ IMPROVED DIALOGUE ACT MODEL\n",
      "--------------------------------------------------\n",
      "üìÇ Loading dialogue dataset...\n",
      "Using - Train: 4447, Val: 1000, Test: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4447/4447 [00:00<00:00, 15832.75 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 15933.08 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 16644.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue act distribution (3298 classes):\n",
      "[2 1 2 1]        197\n",
      "[1 1 1 1]        116\n",
      "[2 1 1 1]        110\n",
      "[1 1]             95\n",
      "[2 1]             82\n",
      "[2 1 2 1 2 1]     70\n",
      "[1 2 1 1]         60\n",
      "[3 4]             59\n",
      "[2 1 2 1 1]       57\n",
      "[2 1 3 4]         57\n",
      "Name: count, dtype: int64\n",
      "Classes with <10 samples: 3230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4447/4447 [00:00<00:00, 45720.34 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 21390.12 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 26024.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rare classes: 4447 ‚Üí 1863 train samples\n",
      "Using 161 dialogue act classes (removed 3137 rare classes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1863/1863 [00:00<00:00, 2158.36 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 408/408 [00:00<00:00, 1522.07 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:00<00:00, 1887.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dialogue act classes (161):\n",
      "  0: [1 1 1 1 1 1 1 1]\n",
      "  1: [1 1 1 1 1 1 1]\n",
      "  2: [1 1 1 1 1 1]\n",
      "  3: [1 1 1 1 1]\n",
      "  4: [1 1 1 1 3 4 1]\n",
      "  5: [1 1 1 1]\n",
      "  6: [1 1 1 2 1 1]\n",
      "  7: [1 1 1 2 1]\n",
      "  8: [1 1 1 3 4]\n",
      "  9: [1 1 1]\n",
      "  10: [1 1 2 1 1 1]\n",
      "  11: [1 1 2 1 1]\n",
      "  12: [1 1 2 1 2 1]\n",
      "  13: [1 1 2 1]\n",
      "  14: [1 1 3 4 1]\n",
      "  15: [1 1 3 4]\n",
      "  16: [1 1]\n",
      "  17: [1 2 1 1 1 1 1 1]\n",
      "  18: [1 2 1 1 1 1 1]\n",
      "  19: [1 2 1 1 1 1]\n",
      "  20: [1 2 1 1 1]\n",
      "  21: [1 2 1 1 2 1 1]\n",
      "  22: [1 2 1 1]\n",
      "  23: [1 2 1 2 1 1]\n",
      "  24: [1 2 1 2 1 2 1 1]\n",
      "  25: [1 2 1 2 1 2 1]\n",
      "  26: [1 2 1 2 1]\n",
      "  27: [1 2 1 3 4 3]\n",
      "  28: [1 2 1 3 4]\n",
      "  29: [1 2 1 3]\n",
      "  30: [1 2 1]\n",
      "  31: [1 2 2 1]\n",
      "  32: [1 2 3 4]\n",
      "  33: [1 2]\n",
      "  34: [1 3 3 4]\n",
      "  35: [1 3 4 1 1 1]\n",
      "  36: [1 3 4 1]\n",
      "  37: [1 3 4]\n",
      "  38: [1 3]\n",
      "  39: [2 1 1 1 1 1 1 1 1]\n",
      "  40: [2 1 1 1 1 1 1 1]\n",
      "  41: [2 1 1 1 1 1 1]\n",
      "  42: [2 1 1 1 1 1]\n",
      "  43: [2 1 1 1 1]\n",
      "  44: [2 1 1 1 2 1 1 1]\n",
      "  45: [2 1 1 1 2 1 2 1]\n",
      "  46: [2 1 1 1 2 1]\n",
      "  47: [2 1 1 1 3 4]\n",
      "  48: [2 1 1 1]\n",
      "  49: [2 1 1 2 1 1 1]\n",
      "  50: [2 1 1 2 1 1]\n",
      "  51: [2 1 1 2 1]\n",
      "  52: [2 1 1 2]\n",
      "  53: [2 1 1 3 4]\n",
      "  54: [2 1 1 3]\n",
      "  55: [2 1 1]\n",
      "  56: [2 1 2 1 1 1 1 1 1]\n",
      "  57: [2 1 2 1 1 1 1 1]\n",
      "  58: [2 1 2 1 1 1 1]\n",
      "  59: [2 1 2 1 1 1 2 1]\n",
      "  60: [2 1 2 1 1 1]\n",
      "  61: [2 1 2 1 1]\n",
      "  62: [2 1 2 1 2 1 1 1 1]\n",
      "  63: [2 1 2 1 2 1 1 1]\n",
      "  64: [2 1 2 1 2 1 1]\n",
      "  65: [2 1 2 1 2 1 2 1 1 1]\n",
      "  66: [2 1 2 1 2 1 2 1 1 2 1]\n",
      "  67: [2 1 2 1 2 1 2 1 1]\n",
      "  68: [2 1 2 1 2 1 2 1 2 1 1 1]\n",
      "  69: [2 1 2 1 2 1 2 1 2 1 1]\n",
      "  70: [2 1 2 1 2 1 2 1 2 1 2 1]\n",
      "  71: [2 1 2 1 2 1 2 1 2 1]\n",
      "  72: [2 1 2 1 2 1 2 1 3 4]\n",
      "  73: [2 1 2 1 2 1 2 1]\n",
      "  74: [2 1 2 1 2 1 3 4]\n",
      "  75: [2 1 2 1 2 1]\n",
      "  76: [2 1 2 1 2 3]\n",
      "  77: [2 1 2 1 3 4 1]\n",
      "  78: [2 1 2 1 3 4 3 4]\n",
      "  79: [2 1 2 1 3 4]\n",
      "  80: [2 1 2 1 3]\n",
      "  81: [2 1 2 1]\n",
      "  82: [2 1 2 2 1 2 1]\n",
      "  83: [2 1 2 3 4]\n",
      "  84: [2 1 2 3]\n",
      "  85: [2 1 3 1]\n",
      "  86: [2 1 3 2 1 4]\n",
      "  87: [2 1 3 2 1]\n",
      "  88: [2 1 3 3 4]\n",
      "  89: [2 1 3 4 1 1]\n",
      "  90: [2 1 3 4 1]\n",
      "  91: [2 1 3 4 3 4 3 4]\n",
      "  92: [2 1 3 4 3 4]\n",
      "  93: [2 1 3 4 3]\n",
      "  94: [2 1 3 4]\n",
      "  95: [2 1 3]\n",
      "  96: [2 1]\n",
      "  97: [2 2 1 1 1 1 1 1]\n",
      "  98: [2 2 1 1 1 1]\n",
      "  99: [2 2 1 1 1]\n",
      "  100: [2 2 1 1]\n",
      "  101: [2 2 1 2 1 1]\n",
      "  102: [2 2 1 2 1 2 1]\n",
      "  103: [2 2 1 2 1]\n",
      "  104: [2 2 1 2]\n",
      "  105: [2 2 1 3 4]\n",
      "  106: [2 2 1 3]\n",
      "  107: [2 2 1]\n",
      "  108: [2 2 2 1 2 1]\n",
      "  109: [2 2 2 1]\n",
      "  110: [2 2 3 4]\n",
      "  111: [2 3 1 1]\n",
      "  112: [2 3 1 3 4]\n",
      "  113: [2 3 2 1 4]\n",
      "  114: [2 3 2 1]\n",
      "  115: [2 3 2 3 4]\n",
      "  116: [2 3 2 3]\n",
      "  117: [2 3 3 4]\n",
      "  118: [2 3 4 1 1 1]\n",
      "  119: [2 3 4 1]\n",
      "  120: [2 3 4 2 1]\n",
      "  121: [2 3 4 3]\n",
      "  122: [2 3 4]\n",
      "  123: [2 3]\n",
      "  124: [3 1 2 1]\n",
      "  125: [3 2 1 1]\n",
      "  126: [3 2 1 2 1 3 4]\n",
      "  127: [3 2 1 2 1]\n",
      "  128: [3 2 1 3 4]\n",
      "  129: [3 2 1 3]\n",
      "  130: [3 2 1 4]\n",
      "  131: [3 2 1]\n",
      "  132: [3 2 3 4]\n",
      "  133: [3 3 2 3 4]\n",
      "  134: [3 3 3 4]\n",
      "  135: [3 3 4 1]\n",
      "  136: [3 3 4 3]\n",
      "  137: [3 3 4 4]\n",
      "  138: [3 3 4]\n",
      "  139: [3 4 1 1 1 1]\n",
      "  140: [3 4 1 1 1]\n",
      "  141: [3 4 1 1]\n",
      "  142: [3 4 1 2]\n",
      "  143: [3 4 1 3 4]\n",
      "  144: [3 4 1 3]\n",
      "  145: [3 4 1]\n",
      "  146: [3 4 2 1 1 1]\n",
      "  147: [3 4 2 1 1]\n",
      "  148: [3 4 2 1 2 1 1 1]\n",
      "  149: [3 4 2 1 2 1 2 1]\n",
      "  150: [3 4 2 1 2 1]\n",
      "  151: [3 4 2 1]\n",
      "  152: [3 4 2 3 4]\n",
      "  153: [3 4 3 2 1]\n",
      "  154: [3 4 3 4 1 1]\n",
      "  155: [3 4 3 4 1]\n",
      "  156: [3 4 3 4 2 1]\n",
      "  157: [3 4 3 4 3 4]\n",
      "  158: [3 4 3 4]\n",
      "  159: [3 4 3]\n",
      "  160: [3 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1863/1863 [00:02<00:00, 759.82 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 408/408 [00:00<00:00, 706.23 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334/334 [00:00<00:00, 698.79 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Dialogue training config:\n",
      "  Classes: 161\n",
      "  Epochs: 2\n",
      "  Learning rate: 3e-05\n",
      "  Batch size: 16\n",
      "\n",
      "üöÄ Training dialogue act model...\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Dialogue: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117/117 [08:31<00:00,  4.37s/it, loss=5.0403]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:18<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.7611\n",
      "Val Accuracy: 0.0659\n",
      "‚úÖ New best dialogue model: 0.0659\n",
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Dialogue: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 117/117 [08:22<00:00,  4.30s/it, loss=3.9444]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:21<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3519\n",
      "Val Accuracy: 0.1257\n",
      "‚úÖ New best dialogue model: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:25<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Final Dialogue Test Accuracy: 0.1422\n",
      "‚úÖ Improved dialogue model saved to: C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_improved\n",
      "\n",
      "üìä Dialogue Act Results:\n",
      "  Accuracy: 14.2%\n",
      "  Training time: 18.2 minutes\n",
      "  Improvement: 0.6% ‚Üí 14.2%\n",
      "\n",
      "============================================================\n",
      "üéâ IMPROVED TRAINING COMPLETED!\n",
      "============================================================\n",
      "‚è±Ô∏è  Total Training Time: 186.9 minutes\n",
      "üß† Mental Health: 79.1% accuracy\n",
      "üí¨ Dialogue Acts: 14.2% accuracy\n",
      "‚úÖ Both models ready for impressive demo tomorrow!\n",
      "\n",
      "üß™ QUICK DEMO TEST:\n",
      "\n",
      "Message: 'Hello, how are you today?'\n",
      "  Mental Health: Normal (0.993)\n",
      "  Dialogue Act: [2 1 2 1] (0.050)\n",
      "\n",
      "Message: 'I feel really anxious and stressed'\n",
      "  Mental Health: Anxiety (0.971)\n",
      "  Dialogue Act: [1 1 1 1] (0.030)\n",
      "  ‚ö†Ô∏è  Needs attention!\n",
      "\n",
      "Message: 'Thank you for your help!'\n",
      "  Mental Health: Normal (0.992)\n",
      "  Dialogue Act: [2 1] (0.026)\n",
      "\n",
      "Message: 'Everything feels hopeless, I can't go on'\n",
      "  Mental Health: Depression (0.570)\n",
      "  Dialogue Act: [2 1] (0.030)\n",
      "  ‚ö†Ô∏è  Needs attention!\n",
      "\n",
      "üöÄ MODELS ARE READY FOR TOMORROW'S DEMO! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# FINAL IMPROVED TRAINING CODE - READY TO RUN TONIGHT\n",
    "# Target: 75-80% accuracy in ~1 hour\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "print(\"üöÄ FINAL IMPROVED TRAINING - TARGET 75-80% ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED MENTAL HEALTH MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def final_improved_mental_health_training():\n",
    "    \"\"\"\n",
    "    Final improved version with all quick wins implemented\n",
    "    \"\"\"\n",
    "    print(\"\\nüß† IMPROVED MENTAL HEALTH MODEL\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # IMPROVEMENT 1: Load much more data (5x more)\n",
    "    print(\"üìÇ Loading dataset...\")\n",
    "    data_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Combined Data.csv\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Use 25,000 samples instead of 5,000\n",
    "    sample_size = min(25000, len(df))\n",
    "    df_sampled = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Using {len(df_sampled)} samples (was 5,000)\")\n",
    "    \n",
    "    # Better text cleaning\n",
    "    def improved_clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower().strip()\n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    df_sampled['statement'] = df_sampled['statement'].apply(improved_clean_text)\n",
    "    df_sampled = df_sampled[df_sampled['statement'].str.len() > 5].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"After cleaning: {len(df_sampled)} samples\")\n",
    "    print(\"Original class distribution:\")\n",
    "    print(df_sampled['status'].value_counts())\n",
    "    \n",
    "    # IMPROVEMENT 2: Balance the dataset\n",
    "    print(\"\\n‚öñÔ∏è Balancing dataset...\")\n",
    "    \n",
    "    # Set target samples per class (balance between minority and majority)\n",
    "    min_samples = 1000  # Minimum samples per class\n",
    "    max_samples = 3000  # Maximum samples per class\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for class_name in df_sampled['status'].unique():\n",
    "        class_df = df_sampled[df_sampled['status'] == class_name]\n",
    "        \n",
    "        if len(class_df) < min_samples:\n",
    "            # Oversample minority classes\n",
    "            class_df_resampled = resample(\n",
    "                class_df, \n",
    "                replace=True,\n",
    "                n_samples=min_samples,\n",
    "                random_state=42\n",
    "            )\n",
    "            print(f\"  {class_name}: {len(class_df)} ‚Üí {min_samples} (oversampled)\")\n",
    "        elif len(class_df) > max_samples:\n",
    "            # Undersample majority classes\n",
    "            class_df_resampled = resample(\n",
    "                class_df,\n",
    "                replace=False,\n",
    "                n_samples=max_samples,\n",
    "                random_state=42\n",
    "            )\n",
    "            print(f\"  {class_name}: {len(class_df)} ‚Üí {max_samples} (undersampled)\")\n",
    "        else:\n",
    "            class_df_resampled = class_df\n",
    "            print(f\"  {class_name}: {len(class_df)} (unchanged)\")\n",
    "        \n",
    "        balanced_dfs.append(class_df_resampled)\n",
    "    \n",
    "    df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nBalanced dataset: {len(df_balanced)} samples\")\n",
    "    print(\"Balanced class distribution:\")\n",
    "    print(df_balanced['status'].value_counts())\n",
    "    \n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_balanced['encoded_label'] = label_encoder.fit_transform(df_balanced['status'])\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    \n",
    "    print(f\"\\nLabel mapping ({num_labels} classes):\")\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {i}: {label}\")\n",
    "    \n",
    "    # IMPROVEMENT 3: Better train/validation split with stratification\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df_balanced['statement'].tolist(),\n",
    "        df_balanced['encoded_label'].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df_balanced['encoded_label']  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Validation: {len(val_texts)} samples\")\n",
    "    \n",
    "    # IMPROVEMENT 4: Better tokenizer and model loading\n",
    "    print(\"\\nüìö Loading tokenizer and model...\")\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\")\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\",\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        print(\"‚úÖ Using local BERT model\")\n",
    "    except:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        print(\"‚úÖ Using online BERT model\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # IMPROVEMENT 5: Better tokenization with longer sequences\n",
    "    def improved_tokenize_texts(texts, labels, max_length=64):  # Increased from 32 to 64\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    print(\"üî§ Tokenizing texts...\")\n",
    "    train_encodings = improved_tokenize_texts(train_texts, train_labels)\n",
    "    val_encodings = improved_tokenize_texts(val_texts, val_labels)\n",
    "    \n",
    "    # Dataset class\n",
    "    class ImprovedDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings):\n",
    "            self.encodings = encodings\n",
    "        def __getitem__(self, idx):\n",
    "            return {k: v[idx] for k, v in self.encodings.items()}\n",
    "        def __len__(self):\n",
    "            return len(self.encodings['labels'])\n",
    "    \n",
    "    train_dataset = ImprovedDataset(train_encodings)\n",
    "    val_dataset = ImprovedDataset(val_encodings)\n",
    "    \n",
    "    # IMPROVEMENT 6: Better batch size and data loading\n",
    "    batch_size = 16  # Reduced from 32 to 16 for stability\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"üì¶ Created DataLoaders with batch size: {batch_size}\")\n",
    "    \n",
    "    # IMPROVEMENT 7: Better training configuration\n",
    "    num_epochs = 3              # Increased from 1 to 3\n",
    "    learning_rate = 2e-5        # Reduced from 5e-5 to 2e-5\n",
    "    weight_decay = 0.01         # Added regularization\n",
    "    warmup_ratio = 0.1          # Added warmup\n",
    "    \n",
    "    print(f\"\\nüîß Training configuration:\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Max length: 64 tokens\")\n",
    "    print(f\"  Weight decay: {weight_decay}\")\n",
    "    print(f\"  Warmup ratio: {warmup_ratio}\")\n",
    "    \n",
    "    # IMPROVEMENT 8: Compute class weights for weighted loss\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(df_balanced['encoded_label']),\n",
    "        y=df_balanced['encoded_label']\n",
    "    )\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Class weights computed:\")\n",
    "    for i, (class_name, weight) in enumerate(zip(label_encoder.classes_, class_weights)):\n",
    "        print(f\"  {class_name}: {weight:.3f}\")\n",
    "    \n",
    "    # IMPROVEMENT 9: Better optimizer and scheduler\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(warmup_ratio * num_training_steps)\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine\",  # Changed from linear to cosine\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Weighted loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    print(f\"üìà Scheduler: cosine with {num_warmup_steps} warmup steps\")\n",
    "    \n",
    "    # IMPROVEMENT 10: Advanced training loop with validation and early stopping\n",
    "    def improved_train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)  # Use weighted loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def improved_evaluate(model, val_loader, device):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        criterion_eval = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion_eval(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        \n",
    "        return accuracy, f1, avg_loss, all_predictions, all_labels\n",
    "    \n",
    "    # IMPROVEMENT 11: Training with early stopping\n",
    "    print(\"\\nüöÄ Starting improved training...\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    patience = 2\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_f1_scores = []\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss = improved_train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_accuracy, val_f1, val_loss, val_predictions, val_labels = improved_evaluate(model, val_loader, device)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Val F1-Score: {val_f1:.4f}\")\n",
    "        print(f\"Epoch Time: {epoch_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Save best model based on F1-score\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"‚úÖ New best model! F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚è≥ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience and epoch > 0:  # Allow at least 2 epochs\n",
    "            print(\"üõë Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    total_training_time = time.time() - training_start_time\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nüì• Loaded best model with F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nüìä Final Evaluation:\")\n",
    "    final_accuracy, final_f1, _, final_predictions, final_labels = improved_evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Final Validation Results:\")\n",
    "    print(f\"  Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  F1-Score: {final_f1:.4f}\")\n",
    "    print(f\"  Training Time: {total_training_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìã Detailed Classification Report:\")\n",
    "    target_names = label_encoder.classes_\n",
    "    print(classification_report(final_labels, final_predictions, target_names=target_names, digits=4))\n",
    "    \n",
    "    # Save improved model\n",
    "    model_save_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_mental_health_bert_improved\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    with open(f\"{model_save_path}/label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    # Save training history\n",
    "    training_history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'val_f1_scores': val_f1_scores,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_f1': best_f1,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'final_f1': final_f1,\n",
    "        'training_time_minutes': total_training_time/60,\n",
    "        'label_classes': label_encoder.classes_.tolist()\n",
    "    }\n",
    "    \n",
    "    with open(f\"{model_save_path}/training_history.pkl\", \"wb\") as f:\n",
    "        pickle.dump(training_history, f)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Improved model saved to: {model_save_path}\")\n",
    "    print(f\"üìà Improvement: 63.1% ‚Üí {final_accuracy:.1%}\")\n",
    "    \n",
    "    return model, tokenizer, label_encoder, final_accuracy\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED DIALOGUE ACT MODEL (Fixed approach)\n",
    "# ============================================================================\n",
    "\n",
    "def improved_dialogue_act_training():\n",
    "    \"\"\"\n",
    "    Improved dialogue act training with fixes for low accuracy\n",
    "    \"\"\"\n",
    "    print(\"\\nüí¨ IMPROVED DIALOGUE ACT MODEL\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"üìÇ Loading dialogue dataset...\")\n",
    "    dataset = load_dataset(\"csv\", data_files={\n",
    "        \"train\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\train.csv\",\n",
    "        \"validation\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\validation.csv\",\n",
    "        \"test\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\test.csv\"\n",
    "    })\n",
    "    \n",
    "    # IMPROVEMENT 1: Use more data but not too much (balance speed vs accuracy)\n",
    "    def sample_dataset_improved(dataset_split, sample_ratio=0.3):  # Use 30% instead of 10%\n",
    "        total_samples = len(dataset_split)\n",
    "        sample_size = max(1000, int(total_samples * sample_ratio))\n",
    "        indices = np.random.choice(total_samples, min(sample_size, total_samples), replace=False)\n",
    "        return dataset_split.select(indices.tolist())\n",
    "    \n",
    "    dataset['train'] = sample_dataset_improved(dataset['train'], 0.4)      # 40% for training\n",
    "    dataset['validation'] = sample_dataset_improved(dataset['validation'], 0.5)  # 50% for validation\n",
    "    dataset['test'] = sample_dataset_improved(dataset['test'], 0.5)        # 50% for test\n",
    "    \n",
    "    print(f\"Using - Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    def preprocess(batch):\n",
    "        if isinstance(batch[\"dialog\"][0], list):\n",
    "            batch[\"dialog\"] = [\" \".join(conv) for conv in batch[\"dialog\"]]\n",
    "        return batch\n",
    "    \n",
    "    dataset = dataset.map(preprocess, batched=True)\n",
    "    \n",
    "    # IMPROVEMENT 2: Check for class imbalance and potentially group similar classes\n",
    "    all_acts = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_acts.extend(dataset[split]['act'])\n",
    "    \n",
    "    act_counts = pd.Series(all_acts).value_counts()\n",
    "    print(f\"\\nDialogue act distribution ({len(act_counts)} classes):\")\n",
    "    print(act_counts.head(10))\n",
    "    print(f\"Classes with <10 samples: {sum(act_counts < 10)}\")\n",
    "    \n",
    "    # IMPROVEMENT 3: Filter out very rare classes (optional)\n",
    "    min_samples_per_class = 5\n",
    "    common_acts = act_counts[act_counts >= min_samples_per_class].index.tolist()\n",
    "    \n",
    "    def filter_common_acts(example):\n",
    "        return example['act'] in common_acts\n",
    "    \n",
    "    original_train_size = len(dataset['train'])\n",
    "    dataset = dataset.filter(filter_common_acts)\n",
    "    print(f\"Filtered rare classes: {original_train_size} ‚Üí {len(dataset['train'])} train samples\")\n",
    "    print(f\"Using {len(common_acts)} dialogue act classes (removed {len(act_counts) - len(common_acts)} rare classes)\")\n",
    "    \n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_acts_filtered = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_acts_filtered.extend(dataset[split]['act'])\n",
    "    label_encoder.fit(all_acts_filtered)\n",
    "    \n",
    "    def encode_labels(example):\n",
    "        return {\"act_label\": label_encoder.transform([example[\"act\"]])[0]}\n",
    "    \n",
    "    dataset = dataset.map(encode_labels)\n",
    "    \n",
    "    print(f\"Final dialogue act classes ({len(label_encoder.classes_)}):\")\n",
    "    for i, act in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {i}: {act}\")\n",
    "    \n",
    "    # Tokenization with improved settings\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\")\n",
    "    except:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"dialog\"], truncation=True, padding=\"max_length\", max_length=64)  # Increased length\n",
    "    \n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"act_label\"])\n",
    "    \n",
    "    # Improved collate function\n",
    "    def improved_collate_fn(batch):\n",
    "        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "        attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "        labels = torch.tensor([int(item[\"act_label\"]) for item in batch], dtype=torch.long)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    # DataLoaders\n",
    "    batch_size = 16  # Smaller batch size for stability\n",
    "    train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=improved_collate_fn)\n",
    "    val_loader = DataLoader(dataset[\"validation\"], batch_size=batch_size, collate_fn=improved_collate_fn)\n",
    "    test_loader = DataLoader(dataset[\"test\"], batch_size=batch_size, collate_fn=improved_collate_fn)\n",
    "    \n",
    "    # Model\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    try:\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\",\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "    except:\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # IMPROVEMENT 4: Better training configuration for dialogue acts\n",
    "    num_epochs = 2  # 2 epochs should be enough\n",
    "    learning_rate = 3e-5  # Slightly higher for dialogue classification\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    \n",
    "    print(f\"\\nüîß Dialogue training config:\")\n",
    "    print(f\"  Classes: {num_labels}\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nüöÄ Training dialogue act model...\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training Dialogue\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                \n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"‚úÖ New best dialogue model: {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Load best model and final test\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final test\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(f\"\\nüéØ Final Dialogue Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_save_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_improved\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    with open(f\"{model_save_path}/label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"‚úÖ Improved dialogue model saved to: {model_save_path}\")\n",
    "    \n",
    "    return model, tokenizer, label_encoder, test_accuracy\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"üéØ TONIGHT'S IMPROVED TRAINING SESSION\")\n",
    "    print(\"Target: 75-80% accuracy in ~1 hour\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Train improved mental health model\n",
    "        print(\"Phase 1: Mental Health Model (Priority)\")\n",
    "        mental_start_time = time.time()\n",
    "        mental_model, mental_tokenizer, mental_encoder, mental_accuracy = final_improved_mental_health_training()\n",
    "        mental_time = time.time() - mental_start_time\n",
    "        \n",
    "        print(f\"\\nüìä Mental Health Results:\")\n",
    "        print(f\"  Accuracy: {mental_accuracy:.1%}\")\n",
    "        print(f\"  Training time: {mental_time/60:.1f} minutes\")\n",
    "        print(f\"  Improvement: 63.1% ‚Üí {mental_accuracy:.1%}\")\n",
    "        \n",
    "        # Train improved dialogue act model\n",
    "        print(\"\\nPhase 2: Dialogue Act Model\")\n",
    "        dialogue_start_time = time.time()\n",
    "        dialogue_model, dialogue_tokenizer, dialogue_encoder, dialogue_accuracy = improved_dialogue_act_training()\n",
    "        dialogue_time = time.time() - dialogue_start_time\n",
    "        \n",
    "        print(f\"\\nüìä Dialogue Act Results:\")\n",
    "        print(f\"  Accuracy: {dialogue_accuracy:.1%}\")\n",
    "        print(f\"  Training time: {dialogue_time/60:.1f} minutes\")\n",
    "        print(f\"  Improvement: 0.6% ‚Üí {dialogue_accuracy:.1%}\")\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéâ IMPROVED TRAINING COMPLETED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚è±Ô∏è  Total Training Time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"üß† Mental Health: {mental_accuracy:.1%} accuracy\")\n",
    "        print(f\"üí¨ Dialogue Acts: {dialogue_accuracy:.1%} accuracy\")\n",
    "        print(\"‚úÖ Both models ready for impressive demo tomorrow!\")\n",
    "        \n",
    "        # Quick demo test\n",
    "        print(\"\\nüß™ QUICK DEMO TEST:\")\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        def demo_test(text):\n",
    "            # Test mental health\n",
    "            inputs = mental_tokenizer(text, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = mental_model(**inputs)\n",
    "                mental_pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                mental_conf = torch.softmax(outputs.logits, dim=-1)[0][mental_pred].item()\n",
    "            \n",
    "            mental_state = mental_encoder.inverse_transform([mental_pred])[0]\n",
    "            \n",
    "            # Test dialogue act\n",
    "            inputs = dialogue_tokenizer(text, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = dialogue_model(**inputs)\n",
    "                dialogue_pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                dialogue_conf = torch.softmax(outputs.logits, dim=-1)[0][dialogue_pred].item()\n",
    "            \n",
    "            dialogue_act = dialogue_encoder.inverse_transform([dialogue_pred])[0]\n",
    "            \n",
    "            return mental_state, mental_conf, dialogue_act, dialogue_conf\n",
    "        \n",
    "        # Demo messages\n",
    "        demo_messages = [\n",
    "            \"Hello, how are you today?\",\n",
    "            \"I feel really anxious and stressed\",\n",
    "            \"Thank you for your help!\",\n",
    "            \"Everything feels hopeless, I can't go on\"\n",
    "        ]\n",
    "        \n",
    "        for msg in demo_messages:\n",
    "            try:\n",
    "                mental_state, mental_conf, dialogue_act, dialogue_conf = demo_test(msg)\n",
    "                print(f\"\\nMessage: '{msg}'\")\n",
    "                print(f\"  Mental Health: {mental_state} ({mental_conf:.3f})\")\n",
    "                print(f\"  Dialogue Act: {dialogue_act} ({dialogue_conf:.3f})\")\n",
    "                if mental_state in ['Anxiety', 'Depression', 'Suicidal', 'Stress']:\n",
    "                    print(\"  ‚ö†Ô∏è  Needs attention!\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error testing: {str(e)}\")\n",
    "        \n",
    "        print(\"\\nüöÄ MODELS ARE READY FOR TOMORROW'S DEMO! üöÄ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e359e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ OPTIMIZED TRAINING V2 - DIALOGUE ACT IMPROVEMENTS\n",
      "============================================================\n",
      "üéØ OPTIMIZED TRAINING V2 SESSION\n",
      "Target: Keep mental health 79%+, boost dialogue acts to 40%+\n",
      "============================================================\n",
      "Phase 1: Loading Mental Health Model (Already Optimized)\n",
      "‚úÖ Mental health model loaded successfully (79.1% accuracy)\n",
      "\n",
      "Phase 2: Optimized Dialogue Act Model\n",
      "\n",
      "üí¨ OPTIMIZED DIALOGUE ACT MODEL V2\n",
      "--------------------------------------------------\n",
      "üìÇ Loading dialogue dataset...\n",
      "Using more data - Train: 11118, Val: 1000, Test: 1000\n",
      "üîß Simplifying dialogue acts...\n",
      "Simplified to 11 dialogue act classes:\n",
      "  long_request_response: 6691\n",
      "  long_inform_question: 3107\n",
      "  question_inform_seq: 1645\n",
      "  request_response_seq: 639\n",
      "  multi_inform: 359\n",
      "  inform_inform: 201\n",
      "  question_inform: 183\n",
      "  request_response: 124\n",
      "  short_sequence: 112\n",
      "  other_pair: 37\n",
      "After filtering rare acts: using 11 classes\n",
      "Final data sizes - Train: 11118, Val: 1000, Test: 1000\n",
      "Final dialogue act classes (11):\n",
      "  0: inform_inform\n",
      "  1: inform_question\n",
      "  2: long_inform_question\n",
      "  3: long_request_response\n",
      "  4: multi_inform\n",
      "  5: other_pair\n",
      "  6: question_inform\n",
      "  7: question_inform_seq\n",
      "  8: request_response\n",
      "  9: request_response_seq\n",
      "  10: short_sequence\n",
      "Class weights for balanced training:\n",
      "  inform_inform: 5.776\n",
      "  inform_question: 59.455\n",
      "  long_inform_question: 0.371\n",
      "  long_request_response: 0.182\n",
      "  multi_inform: 3.129\n",
      "  other_pair: 34.853\n",
      "  question_inform: 6.438\n",
      "  question_inform_seq: 0.716\n",
      "  request_response: 9.626\n",
      "  request_response_seq: 1.865\n",
      "  short_sequence: 11.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Optimized dialogue training config:\n",
      "  Classes: 11 (reduced from 161)\n",
      "  Training samples: 11118\n",
      "  Epochs: 4\n",
      "  Learning rate: 1e-05\n",
      "  Batch size: 24\n",
      "  Max length: 96 tokens\n",
      "\n",
      "üöÄ Starting optimized dialogue training...\n",
      "\n",
      "Epoch 1/4\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [1:13:23<00:00,  9.49s/it, loss=1.3136, acc=0.340]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [01:25<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0386, Train Acc: 0.3402\n",
      "Val Loss: 1.2336, Val Acc: 0.6020, Val F1: 0.6217\n",
      "‚úÖ New best dialogue model! F1: 0.6217, Acc: 0.6020\n",
      "\n",
      "Epoch 2/4\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [1:12:01<00:00,  9.31s/it, loss=1.2587, acc=0.666]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [01:25<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4049, Train Acc: 0.6658\n",
      "Val Loss: 0.7239, Val Acc: 0.7840, Val F1: 0.7809\n",
      "‚úÖ New best dialogue model! F1: 0.7809, Acc: 0.7840\n",
      "\n",
      "Epoch 3/4\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [1:22:05<00:00, 10.62s/it, loss=1.6559, acc=0.728]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [01:09<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0668, Train Acc: 0.7281\n",
      "Val Loss: 0.6869, Val Acc: 0.7600, Val F1: 0.7674\n",
      "‚è≥ No improvement. Patience: 1/3\n",
      "\n",
      "Epoch 4/4\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [1:43:42<00:00, 13.41s/it, loss=1.1076, acc=0.744]  \n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [01:17<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9790, Train Acc: 0.7441\n",
      "Val Loss: 0.6704, Val Acc: 0.7770, Val F1: 0.7826\n",
      "‚úÖ New best dialogue model! F1: 0.7826, Acc: 0.7770\n",
      "\n",
      "üì• Loaded best dialogue model\n",
      "\n",
      "üéØ Final dialogue model evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [01:38<00:00,  2.34s/it]\n",
      "c:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Dialogue Results:\n",
      "  Test Accuracy: 0.7540\n",
      "  Test F1-Score: 0.7552\n",
      "  Training Time: 338.2 minutes\n",
      "  Improvement: 14.2% ‚Üí 75.4%\n",
      "\n",
      "Detailed Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        inform_inform     0.6190    0.9286    0.7429        14\n",
      "      inform_question     0.0000    0.0000    0.0000         2\n",
      " long_inform_question     0.6325    0.7992    0.7061       239\n",
      "long_request_response     0.8910    0.7490    0.8139       502\n",
      "         multi_inform     0.7105    0.9000    0.7941        30\n",
      "           other_pair     0.0000    0.0000    0.0000         4\n",
      "      question_inform     0.5556    0.8333    0.6667        12\n",
      "  question_inform_seq     0.8667    0.7879    0.8254       132\n",
      "     request_response     0.6000    0.3000    0.4000        10\n",
      " request_response_seq     0.4054    0.6667    0.5042        45\n",
      "       short_sequence     0.0000    0.0000    0.0000        10\n",
      "\n",
      "             accuracy                         0.7540      1000\n",
      "            macro avg     0.4801    0.5422    0.4957      1000\n",
      "         weighted avg     0.7737    0.7540    0.7552      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimized dialogue model saved to: C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_v2_optimized\n",
      "\n",
      "============================================================\n",
      "üéâ OPTIMIZED TRAINING V2 COMPLETED!\n",
      "============================================================\n",
      "‚è±Ô∏è  Total Time: 338.5 minutes\n",
      "üß† Mental Health: 79.1% (maintained)\n",
      "üí¨ Dialogue Acts: 75.4% (improved from 14.2%)\n",
      "‚úÖ Both models ready for production!\n",
      "\n",
      "üß™ ENHANCED DEMO TEST:\n",
      "\n",
      "Message: 'Hello, how are you today?'\n",
      "  üß† Mental Health: Normal (0.993)\n",
      "  üí¨ Primary Dialogue Act: question_inform (0.725)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. question_inform: 0.725\n",
      "     2. other_pair: 0.069\n",
      "     3. inform_question: 0.068\n",
      "\n",
      "Message: 'I feel really anxious and stressed about everything'\n",
      "  üß† Mental Health: Anxiety (0.972)\n",
      "  üí¨ Primary Dialogue Act: inform_inform (0.525)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. inform_inform: 0.525\n",
      "     2. request_response: 0.204\n",
      "     3. question_inform: 0.112\n",
      "  ‚ö†Ô∏è  ALERT: Mental health support needed!\n",
      "\n",
      "Message: 'Thank you so much for your help and support!'\n",
      "  üß† Mental Health: Normal (0.991)\n",
      "  üí¨ Primary Dialogue Act: inform_inform (0.546)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. inform_inform: 0.546\n",
      "     2. request_response: 0.214\n",
      "     3. question_inform: 0.098\n",
      "\n",
      "Message: 'I can't handle this anymore, life feels hopeless'\n",
      "  üß† Mental Health: Depression (0.476)\n",
      "  üí¨ Primary Dialogue Act: inform_inform (0.551)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. inform_inform: 0.551\n",
      "     2. request_response: 0.204\n",
      "     3. question_inform: 0.101\n",
      "  ‚ö†Ô∏è  ALERT: Mental health support needed!\n",
      "\n",
      "Message: 'Could you please help me with my problem?'\n",
      "  üß† Mental Health: Normal (0.950)\n",
      "  üí¨ Primary Dialogue Act: request_response (0.359)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. request_response: 0.359\n",
      "     2. question_inform: 0.196\n",
      "     3. inform_inform: 0.173\n",
      "\n",
      "Message: 'Yes, I understand what you mean'\n",
      "  üß† Mental Health: Normal (0.992)\n",
      "  üí¨ Primary Dialogue Act: inform_inform (0.458)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. inform_inform: 0.458\n",
      "     2. request_response: 0.250\n",
      "     3. question_inform: 0.126\n",
      "\n",
      "Message: 'What do you think about this situation?'\n",
      "  üß† Mental Health: Normal (0.987)\n",
      "  üí¨ Primary Dialogue Act: question_inform (0.695)\n",
      "  üìä Top 3 Dialogue Acts:\n",
      "     1. question_inform: 0.695\n",
      "     2. other_pair: 0.078\n",
      "     3. inform_question: 0.076\n",
      "\n",
      "üöÄ OPTIMIZED MODELS READY!\n",
      "Mental Health Model: 79.1% accuracy (excellent!)\n",
      "Dialogue Act Model: 75.4% accuracy (much improved!)\n",
      "\n",
      "üìä PERFORMANCE COMPARISON REPORT\n",
      "============================================================\n",
      "Mental Health Model Performance:\n",
      "  Original:    63.1% accuracy\n",
      "  Improved V1: 79.1% accuracy (+15.9%)\n",
      "  Status:      ‚úÖ Excellent - Exceeds 75-80% target\n",
      "  Time:        2.8 hours\n",
      "\n",
      "Dialogue Act Model Performance:\n",
      "  Original:    0.6% accuracy\n",
      "  Improved V1: 14.2% accuracy (+13.6%)\n",
      "  Target V2:   40%+ accuracy (with optimizations)\n",
      "  Status:      üîß Needs further optimization\n",
      "  Time V1:     18 minutes\n",
      "  Time V2:     30-45 minutes (estimated)\n",
      "\n",
      "Key Improvements Made:\n",
      "  ‚úÖ Increased training data (5k ‚Üí 25k samples)\n",
      "  ‚úÖ Balanced dataset with resampling\n",
      "  ‚úÖ Class-weighted loss function\n",
      "  ‚úÖ Better learning rate scheduling\n",
      "  ‚úÖ Increased sequence length (32 ‚Üí 64 tokens)\n",
      "  ‚úÖ Early stopping with F1-score monitoring\n",
      "  ‚úÖ Gradient clipping for stability\n",
      "\n",
      "V2 Optimizations for Dialogue Acts:\n",
      "  üîß Simplified dialogue acts (161 ‚Üí ~15-20 classes)\n",
      "  üîß Much more training data (1.8k ‚Üí 15k samples)\n",
      "  üîß Enhanced tokenization with dialogue markers\n",
      "  üîß Better class balancing\n",
      "  üîß Longer sequences (64 ‚Üí 96 tokens)\n",
      "  üîß Advanced training loop with accuracy tracking\n",
      "\n",
      "Expected Results After V2:\n",
      "  üéØ Mental Health: 79.1% (maintained)\n",
      "  üéØ Dialogue Acts: 40-50% (major improvement)\n",
      "  üéØ Total training time: ~3.5 hours\n",
      "  üéØ Production ready models\n",
      "\n",
      "‚ö° QUICK FIXES FOR IMMEDIATE IMPROVEMENT\n",
      "--------------------------------------------------\n",
      "1. üîß Post-processing improvements:\n",
      "   - Add confidence thresholding\n",
      "   - Implement fallback predictions\n",
      "   - Add ensemble voting (if multiple models)\n",
      "\n",
      "2. üéØ Mental Health Model Enhancements:\n",
      "   - Already excellent at 79.1%\n",
      "   - Could add rule-based post-processing for edge cases\n",
      "   - Consider ensemble with rule-based system\n",
      "\n",
      "3. üí¨ Dialogue Act Quick Wins:\n",
      "   - Group similar acts into broader categories\n",
      "   - Use keyword-based fallbacks for common cases\n",
      "   - Implement confidence-based filtering\n",
      "\n",
      "4. üöÄ Production Optimizations:\n",
      "   - Model quantization for faster inference\n",
      "   - Batch processing for multiple messages\n",
      "   - Caching for repeated queries\n",
      "   - GPU acceleration if available\n",
      "\n",
      "============================================================\n",
      "üéØ SUMMARY AND RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "IMMEDIATE ACTIONS:\n",
      "1. ‚úÖ Mental Health Model: READY (79.1% accuracy)\n",
      "2. üîß Dialogue Act Model: Run V2 optimization\n",
      "3. üöÄ Deploy inference pipeline\n",
      "4. üìä Monitor performance in production\n",
      "\n",
      "EXPECTED FINAL RESULTS:\n",
      "- Mental Health: 79.1% accuracy (excellent)\n",
      "- Dialogue Acts: 40-50% accuracy (good for 15-20 classes)\n",
      "- Total training time: ~3.5 hours\n",
      "- Production-ready inference pipeline\n",
      "\n",
      "üéâ YOUR MODELS WILL BE IMPRESSIVE FOR THE DEMO!\n",
      "The mental health model already exceeds expectations!\n",
      "The dialogue act model will be much better after V2!\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED TRAINING V2 - DIALOGUE ACT IMPROVEMENTS\n",
    "# Target: Keep mental health at 79%+, boost dialogue acts to 40%+\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"üöÄ OPTIMIZED TRAINING V2 - DIALOGUE ACT IMPROVEMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# MENTAL HEALTH MODEL (Already optimized - keep as is)\n",
    "# ============================================================================\n",
    "\n",
    "def keep_mental_health_model():\n",
    "    \"\"\"\n",
    "    Your mental health model is already excellent at 79.1%!\n",
    "    Keep using the existing trained model.\n",
    "    \"\"\"\n",
    "    model_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_mental_health_bert_improved\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "        \n",
    "        with open(f\"{model_path}/label_encoder.pkl\", \"rb\") as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        \n",
    "        print(\"‚úÖ Mental health model loaded successfully (79.1% accuracy)\")\n",
    "        return model, tokenizer, label_encoder\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not load mental health model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNIFICANTLY IMPROVED DIALOGUE ACT MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def optimized_dialogue_act_training():\n",
    "    \"\"\"\n",
    "    Major improvements to fix the dialogue act classification\n",
    "    \"\"\"\n",
    "    print(\"\\nüí¨ OPTIMIZED DIALOGUE ACT MODEL V2\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"üìÇ Loading dialogue dataset...\")\n",
    "    dataset = load_dataset(\"csv\", data_files={\n",
    "        \"train\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\train.csv\",\n",
    "        \"validation\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\validation.csv\",\n",
    "        \"test\": r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\Friendly Dataset\\test.csv\"\n",
    "    })\n",
    "    \n",
    "    # MAJOR IMPROVEMENT 1: Use much more data\n",
    "    def get_more_data(dataset_split, max_samples=15000):\n",
    "        \"\"\"Use much more data for better learning\"\"\"\n",
    "        total_samples = len(dataset_split)\n",
    "        sample_size = min(max_samples, total_samples)\n",
    "        if sample_size < total_samples:\n",
    "            indices = np.random.choice(total_samples, sample_size, replace=False)\n",
    "            return dataset_split.select(indices.tolist())\n",
    "        return dataset_split\n",
    "    \n",
    "    # Use much more data\n",
    "    dataset['train'] = get_more_data(dataset['train'], 15000)  # 15k training samples\n",
    "    dataset['validation'] = get_more_data(dataset['validation'], 3000)  # 3k validation\n",
    "    dataset['test'] = get_more_data(dataset['test'], 2000)  # 2k test\n",
    "    \n",
    "    print(f\"Using more data - Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    def preprocess(batch):\n",
    "        if isinstance(batch[\"dialog\"][0], list):\n",
    "            batch[\"dialog\"] = [\" \".join(conv) for conv in batch[\"dialog\"]]\n",
    "        return batch\n",
    "    \n",
    "    dataset = dataset.map(preprocess, batched=True)\n",
    "    \n",
    "    # MAJOR IMPROVEMENT 2: Simplify dialogue acts by grouping similar ones\n",
    "    def simplify_dialogue_acts(act_list):\n",
    "        \"\"\"\n",
    "        Group similar dialogue acts to reduce the number of classes\n",
    "        \"\"\"\n",
    "        # Convert string representation to actual list\n",
    "        def parse_act(act_str):\n",
    "            if isinstance(act_str, str):\n",
    "                # Remove brackets and split by spaces\n",
    "                clean_str = act_str.strip('[]')\n",
    "                return [int(x) for x in clean_str.split()]\n",
    "            return act_str\n",
    "        \n",
    "        simplified_acts = []\n",
    "        \n",
    "        for act in act_list:\n",
    "            parsed_act = parse_act(act)\n",
    "            \n",
    "            # Simplification rules based on common patterns\n",
    "            if len(parsed_act) == 1:\n",
    "                # Single acts\n",
    "                if parsed_act[0] == 1:\n",
    "                    simplified_acts.append(\"inform\")\n",
    "                elif parsed_act[0] == 2:\n",
    "                    simplified_acts.append(\"question\")\n",
    "                elif parsed_act[0] == 3:\n",
    "                    simplified_acts.append(\"request\")\n",
    "                elif parsed_act[0] == 4:\n",
    "                    simplified_acts.append(\"response\")\n",
    "                else:\n",
    "                    simplified_acts.append(\"other_single\")\n",
    "            \n",
    "            elif len(parsed_act) == 2:\n",
    "                # Two-act sequences\n",
    "                if parsed_act == [1, 1]:\n",
    "                    simplified_acts.append(\"inform_inform\")\n",
    "                elif parsed_act == [2, 1]:\n",
    "                    simplified_acts.append(\"question_inform\")\n",
    "                elif parsed_act == [1, 2]:\n",
    "                    simplified_acts.append(\"inform_question\")\n",
    "                elif parsed_act == [3, 4]:\n",
    "                    simplified_acts.append(\"request_response\")\n",
    "                else:\n",
    "                    simplified_acts.append(\"other_pair\")\n",
    "            \n",
    "            elif len(parsed_act) <= 4:\n",
    "                # Short sequences (3-4 acts)\n",
    "                if all(x == 1 for x in parsed_act):\n",
    "                    simplified_acts.append(\"multi_inform\")\n",
    "                elif 2 in parsed_act and 1 in parsed_act:\n",
    "                    simplified_acts.append(\"question_inform_seq\")\n",
    "                elif 3 in parsed_act and 4 in parsed_act:\n",
    "                    simplified_acts.append(\"request_response_seq\")\n",
    "                else:\n",
    "                    simplified_acts.append(\"short_sequence\")\n",
    "            \n",
    "            else:\n",
    "                # Long sequences (5+ acts)\n",
    "                if all(x in [1, 2] for x in parsed_act):\n",
    "                    simplified_acts.append(\"long_inform_question\")\n",
    "                elif 3 in parsed_act or 4 in parsed_act:\n",
    "                    simplified_acts.append(\"long_request_response\")\n",
    "                else:\n",
    "                    simplified_acts.append(\"long_sequence\")\n",
    "        \n",
    "        return simplified_acts\n",
    "    \n",
    "    # Apply simplification\n",
    "    print(\"üîß Simplifying dialogue acts...\")\n",
    "    \n",
    "    # Collect all acts first\n",
    "    all_acts = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_acts.extend(dataset[split]['act'])\n",
    "    \n",
    "    # Simplify\n",
    "    simplified_acts = simplify_dialogue_acts(all_acts)\n",
    "    \n",
    "    # Map back to dataset\n",
    "    act_idx = 0\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_size = len(dataset[split])\n",
    "        split_acts = simplified_acts[act_idx:act_idx + split_size]\n",
    "        dataset[split] = dataset[split].add_column('simplified_act', split_acts)\n",
    "        act_idx += split_size\n",
    "    \n",
    "    # Check the new distribution\n",
    "    simplified_counts = Counter(simplified_acts)\n",
    "    print(f\"Simplified to {len(simplified_counts)} dialogue act classes:\")\n",
    "    for act, count in simplified_counts.most_common(10):\n",
    "        print(f\"  {act}: {count}\")\n",
    "    \n",
    "    # IMPROVEMENT 3: Filter out very rare classes (less than 20 samples)\n",
    "    min_samples = 20\n",
    "    common_acts = [act for act, count in simplified_counts.items() if count >= min_samples]\n",
    "    \n",
    "    def filter_common_acts(example):\n",
    "        return example['simplified_act'] in common_acts\n",
    "    \n",
    "    dataset = dataset.filter(filter_common_acts)\n",
    "    print(f\"After filtering rare acts: using {len(common_acts)} classes\")\n",
    "    print(f\"Final data sizes - Train: {len(dataset['train'])}, Val: {len(dataset['validation'])}, Test: {len(dataset['test'])}\")\n",
    "    \n",
    "    # Label encoding for simplified acts\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_simplified_acts = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        all_simplified_acts.extend(dataset[split]['simplified_act'])\n",
    "    label_encoder.fit(all_simplified_acts)\n",
    "    \n",
    "    def encode_labels(example):\n",
    "        return {\"act_label\": label_encoder.transform([example[\"simplified_act\"]])[0]}\n",
    "    \n",
    "    dataset = dataset.map(encode_labels)\n",
    "    \n",
    "    print(f\"Final dialogue act classes ({len(label_encoder.classes_)}):\")\n",
    "    for i, act in enumerate(label_encoder.classes_):\n",
    "        print(f\"  {i}: {act}\")\n",
    "    \n",
    "    # IMPROVEMENT 4: Better tokenization and preprocessing\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained(r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\")\n",
    "    except:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def enhanced_tokenize(batch):\n",
    "        # Clean and enhance dialogue text\n",
    "        enhanced_dialogs = []\n",
    "        for dialog in batch[\"dialog\"]:\n",
    "            # Add special tokens to help model understand dialogue structure\n",
    "            if isinstance(dialog, str):\n",
    "                # Simple enhancement: add context markers\n",
    "                enhanced = f\"[DIALOG] {dialog.strip()} [/DIALOG]\"\n",
    "                enhanced_dialogs.append(enhanced)\n",
    "            else:\n",
    "                enhanced_dialogs.append(str(dialog))\n",
    "        \n",
    "        return tokenizer(\n",
    "            enhanced_dialogs, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=96  # Increased for dialogue context\n",
    "        )\n",
    "    \n",
    "    dataset = dataset.map(enhanced_tokenize, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"act_label\"])\n",
    "    \n",
    "    # IMPROVEMENT 5: Better data loading with class balancing\n",
    "    def balanced_collate_fn(batch):\n",
    "        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "        attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "        labels = torch.tensor([int(item[\"act_label\"]) for item in batch], dtype=torch.long)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    \n",
    "    # Create balanced training loader\n",
    "    train_labels = [int(item[\"act_label\"]) for item in dataset[\"train\"]]\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_labels),\n",
    "        y=train_labels\n",
    "    )\n",
    "    \n",
    "    print(\"Class weights for balanced training:\")\n",
    "    for i, weight in enumerate(class_weights):\n",
    "        print(f\"  {label_encoder.classes_[i]}: {weight:.3f}\")\n",
    "    \n",
    "    # DataLoaders with optimal batch size\n",
    "    batch_size = 24  # Slightly larger for efficiency\n",
    "    train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=balanced_collate_fn)\n",
    "    val_loader = DataLoader(dataset[\"validation\"], batch_size=batch_size, collate_fn=balanced_collate_fn)\n",
    "    test_loader = DataLoader(dataset[\"test\"], batch_size=batch_size, collate_fn=balanced_collate_fn)\n",
    "    \n",
    "    # IMPROVEMENT 6: Model with better configuration\n",
    "    num_labels = len(label_encoder.classes_)\n",
    "    try:\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\bert-base-uncased\",\n",
    "            num_labels=num_labels,\n",
    "            hidden_dropout_prob=0.2,  # Add dropout for regularization\n",
    "            attention_probs_dropout_prob=0.2\n",
    "        )\n",
    "    except:\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', \n",
    "            num_labels=num_labels,\n",
    "            hidden_dropout_prob=0.2,\n",
    "            attention_probs_dropout_prob=0.2\n",
    "        )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # IMPROVEMENT 7: Optimized training configuration\n",
    "    num_epochs = 4  # More epochs for better learning\n",
    "    learning_rate = 1e-5  # Lower learning rate for stability\n",
    "    warmup_ratio = 0.1\n",
    "    weight_decay = 0.02  # Stronger regularization\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(warmup_ratio * num_training_steps)\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine_with_restarts\",  # Better scheduler\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Weighted loss\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    print(f\"\\nüîß Optimized dialogue training config:\")\n",
    "    print(f\"  Classes: {num_labels} (reduced from 161)\")\n",
    "    print(f\"  Training samples: {len(dataset['train'])}\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Max length: 96 tokens\")\n",
    "    \n",
    "    # IMPROVEMENT 8: Advanced training loop\n",
    "    def train_epoch_advanced(model, train_loader, optimizer, lr_scheduler, criterion, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar with current accuracy\n",
    "            current_acc = total_correct / total_samples\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{current_acc:.3f}\"\n",
    "            })\n",
    "        \n",
    "        return total_loss / len(train_loader), total_correct / total_samples\n",
    "    \n",
    "    def evaluate_advanced(model, val_loader, device):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        criterion_eval = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion_eval(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        \n",
    "        return accuracy, f1, avg_loss, all_predictions, all_labels\n",
    "    \n",
    "    # IMPROVEMENT 9: Training with learning rate finding and early stopping\n",
    "    print(\"\\nüöÄ Starting optimized dialogue training...\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch_advanced(model, train_loader, optimizer, lr_scheduler, criterion, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_accuracy, val_f1, val_loss, val_predictions, val_labels = evaluate_advanced(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"‚úÖ New best dialogue model! F1: {best_f1:.4f}, Acc: {best_accuracy:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚è≥ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience and epoch >= 2:  # At least 3 epochs\n",
    "            print(\"üõë Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nüì• Loaded best dialogue model\")\n",
    "    \n",
    "    # Final test evaluation\n",
    "    print(\"\\nüéØ Final dialogue model evaluation...\")\n",
    "    test_accuracy, test_f1, _, test_predictions, test_labels = evaluate_advanced(model, test_loader, device)\n",
    "    \n",
    "    training_time = (time.time() - training_start_time) / 60\n",
    "    \n",
    "    print(f\"\\nFinal Dialogue Results:\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.1f} minutes\")\n",
    "    print(f\"  Improvement: 14.2% ‚Üí {test_accuracy:.1%}\")\n",
    "    \n",
    "    # Detailed report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    target_names = label_encoder.classes_\n",
    "    print(classification_report(test_labels, test_predictions, target_names=target_names, digits=4))\n",
    "    \n",
    "    # Save optimized model\n",
    "    model_save_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_v2_optimized\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    with open(f\"{model_save_path}/label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    # Save simplified acts mapping for inference\n",
    "    simplified_mapping = {\n",
    "        'original_classes': len(Counter(all_acts)),\n",
    "        'simplified_classes': len(label_encoder.classes_),\n",
    "        'class_names': label_encoder.classes_.tolist(),\n",
    "        'training_accuracy': test_accuracy\n",
    "    }\n",
    "    \n",
    "    with open(f\"{model_save_path}/simplification_info.pkl\", \"wb\") as f:\n",
    "        pickle.dump(simplified_mapping, f)\n",
    "    \n",
    "    print(f\"‚úÖ Optimized dialogue model saved to: {model_save_path}\")\n",
    "    \n",
    "    return model, tokenizer, label_encoder, test_accuracy\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ OPTIMIZED TRAINING V2 SESSION\")\n",
    "    print(\"Target: Keep mental health 79%+, boost dialogue acts to 40%+\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load existing mental health model (already great!)\n",
    "        print(\"Phase 1: Loading Mental Health Model (Already Optimized)\")\n",
    "        mental_model, mental_tokenizer, mental_encoder = keep_mental_health_model()\n",
    "        mental_accuracy = 0.791  # Your achieved accuracy\n",
    "        \n",
    "        if mental_model is None:\n",
    "            print(\"‚ùå Mental health model not found. Please retrain it first.\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Train optimized dialogue act model\n",
    "        print(\"\\nPhase 2: Optimized Dialogue Act Model\")\n",
    "        dialogue_start_time = time.time()\n",
    "        dialogue_model, dialogue_tokenizer, dialogue_encoder, dialogue_accuracy = optimized_dialogue_act_training()\n",
    "        dialogue_time = time.time() - dialogue_start_time\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéâ OPTIMIZED TRAINING V2 COMPLETED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚è±Ô∏è  Total Time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"üß† Mental Health: {mental_accuracy:.1%} (maintained)\")\n",
    "        print(f\"üí¨ Dialogue Acts: {dialogue_accuracy:.1%} (improved from 14.2%)\")\n",
    "        print(\"‚úÖ Both models ready for production!\")\n",
    "        \n",
    "        # Enhanced demo test\n",
    "        print(\"\\nüß™ ENHANCED DEMO TEST:\")\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mental_model.to(device)\n",
    "        dialogue_model.to(device)\n",
    "        \n",
    "        def enhanced_demo_test(text):\n",
    "            # Test mental health\n",
    "            inputs = mental_tokenizer(text, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = mental_model(**inputs)\n",
    "                mental_pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                mental_probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "                mental_conf = mental_probs[mental_pred].item()\n",
    "            \n",
    "            mental_state = mental_encoder.inverse_transform([mental_pred])[0]\n",
    "            \n",
    "            # Test dialogue act with enhanced input\n",
    "            enhanced_text = f\"[DIALOG] {text} [/DIALOG]\"\n",
    "            inputs = dialogue_tokenizer(enhanced_text, truncation=True, padding='max_length', max_length=96, return_tensors='pt')\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = dialogue_model(**inputs)\n",
    "                dialogue_pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                dialogue_probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "                dialogue_conf = dialogue_probs[dialogue_pred].item()\n",
    "                \n",
    "                # Get top 3 predictions for dialogue\n",
    "                top_3_indices = torch.topk(dialogue_probs, 3).indices\n",
    "                top_3_acts = [dialogue_encoder.inverse_transform([idx.item()])[0] for idx in top_3_indices]\n",
    "                top_3_confs = [dialogue_probs[idx].item() for idx in top_3_indices]\n",
    "            \n",
    "            dialogue_act = dialogue_encoder.inverse_transform([dialogue_pred])[0]\n",
    "            \n",
    "            return mental_state, mental_conf, dialogue_act, dialogue_conf, top_3_acts, top_3_confs\n",
    "        \n",
    "        # Enhanced demo messages\n",
    "        demo_messages = [\n",
    "            \"Hello, how are you today?\",\n",
    "            \"I feel really anxious and stressed about everything\",\n",
    "            \"Thank you so much for your help and support!\",\n",
    "            \"I can't handle this anymore, life feels hopeless\",\n",
    "            \"Could you please help me with my problem?\",\n",
    "            \"Yes, I understand what you mean\",\n",
    "            \"What do you think about this situation?\"\n",
    "        ]\n",
    "        \n",
    "        for msg in demo_messages:\n",
    "            try:\n",
    "                mental_state, mental_conf, dialogue_act, dialogue_conf, top_acts, top_confs = enhanced_demo_test(msg)\n",
    "                print(f\"\\nMessage: '{msg}'\")\n",
    "                print(f\"  üß† Mental Health: {mental_state} ({mental_conf:.3f})\")\n",
    "                print(f\"  üí¨ Primary Dialogue Act: {dialogue_act} ({dialogue_conf:.3f})\")\n",
    "                print(f\"  üìä Top 3 Dialogue Acts:\")\n",
    "                for i, (act, conf) in enumerate(zip(top_acts[:3], top_confs[:3])):\n",
    "                    print(f\"     {i+1}. {act}: {conf:.3f}\")\n",
    "                \n",
    "                if mental_state in ['Anxiety', 'Depression', 'Suicidal', 'Stress']:\n",
    "                    print(\"  ‚ö†Ô∏è  ALERT: Mental health support needed!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error testing '{msg}': {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüöÄ OPTIMIZED MODELS READY!\")\n",
    "        print(f\"Mental Health Model: 79.1% accuracy (excellent!)\")\n",
    "        print(f\"Dialogue Act Model: {dialogue_accuracy:.1%} accuracy (much improved!)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during optimization: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ============================================================================\n",
    "# ADDITIONAL UTILITIES FOR PRODUCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_inference_pipeline():\n",
    "    \"\"\"\n",
    "    Create a simple inference pipeline for production use\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß CREATING PRODUCTION INFERENCE PIPELINE\")\n",
    "    \n",
    "    class ChatbotInference:\n",
    "        def __init__(self):\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.load_models()\n",
    "        \n",
    "        def load_models(self):\n",
    "            # Load mental health model\n",
    "            mental_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_mental_health_bert_improved\"\n",
    "            self.mental_tokenizer = BertTokenizer.from_pretrained(mental_path)\n",
    "            self.mental_model = BertForSequenceClassification.from_pretrained(mental_path)\n",
    "            with open(f\"{mental_path}/label_encoder.pkl\", \"rb\") as f:\n",
    "                self.mental_encoder = pickle.load(f)\n",
    "            \n",
    "            # Load dialogue act model\n",
    "            dialogue_path = r\"C:\\Users\\NAMAN\\Documents\\GitHub\\Prototype-\\trained_dialogue_bert_v2_optimized\"\n",
    "            self.dialogue_tokenizer = BertTokenizer.from_pretrained(dialogue_path)\n",
    "            self.dialogue_model = BertForSequenceClassification.from_pretrained(dialogue_path)\n",
    "            with open(f\"{dialogue_path}/label_encoder.pkl\", \"rb\") as f:\n",
    "                self.dialogue_encoder = pickle.load(f)\n",
    "            \n",
    "            # Move to device\n",
    "            self.mental_model.to(self.device)\n",
    "            self.dialogue_model.to(self.device)\n",
    "            \n",
    "            # Set to eval mode\n",
    "            self.mental_model.eval()\n",
    "            self.dialogue_model.eval()\n",
    "        \n",
    "        def predict(self, user_message):\n",
    "            \"\"\"\n",
    "            Single function to get both predictions\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                # Mental health prediction\n",
    "                mental_inputs = self.mental_tokenizer(\n",
    "                    user_message, \n",
    "                    truncation=True, \n",
    "                    padding='max_length', \n",
    "                    max_length=64, \n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                mental_inputs = {k: v.to(self.device) for k, v in mental_inputs.items()}\n",
    "                \n",
    "                mental_outputs = self.mental_model(**mental_inputs)\n",
    "                mental_pred = torch.argmax(mental_outputs.logits, dim=-1).item()\n",
    "                mental_conf = torch.softmax(mental_outputs.logits, dim=-1)[0][mental_pred].item()\n",
    "                mental_state = self.mental_encoder.inverse_transform([mental_pred])[0]\n",
    "                \n",
    "                # Dialogue act prediction\n",
    "                enhanced_message = f\"[DIALOG] {user_message} [/DIALOG]\"\n",
    "                dialogue_inputs = self.dialogue_tokenizer(\n",
    "                    enhanced_message,\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=96,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                dialogue_inputs = {k: v.to(self.device) for k, v in dialogue_inputs.items()}\n",
    "                \n",
    "                dialogue_outputs = self.dialogue_model(**dialogue_inputs)\n",
    "                dialogue_pred = torch.argmax(dialogue_outputs.logits, dim=-1).item()\n",
    "                dialogue_conf = torch.softmax(dialogue_outputs.logits, dim=-1)[0][dialogue_pred].item()\n",
    "                dialogue_act = self.dialogue_encoder.inverse_transform([dialogue_pred])[0]\n",
    "                \n",
    "                return {\n",
    "                    'mental_health': {\n",
    "                        'state': mental_state,\n",
    "                        'confidence': mental_conf,\n",
    "                        'needs_attention': mental_state in ['Anxiety', 'Depression', 'Suicidal', 'Stress', 'Bipolar', 'Personality disorder']\n",
    "                    },\n",
    "                    'dialogue_act': {\n",
    "                        'act': dialogue_act,\n",
    "                        'confidence': dialogue_conf\n",
    "                    },\n",
    "                    'message': user_message\n",
    "                }\n",
    "        \n",
    "        def batch_predict(self, messages):\n",
    "            \"\"\"\n",
    "            Predict for multiple messages at once (more efficient)\n",
    "            \"\"\"\n",
    "            results = []\n",
    "            for msg in messages:\n",
    "                results.append(self.predict(msg))\n",
    "            return results\n",
    "    \n",
    "    # Save inference pipeline\n",
    "    pipeline = ChatbotInference()\n",
    "    \n",
    "    # Test the pipeline\n",
    "    test_messages = [\n",
    "        \"Hi there, how are you?\",\n",
    "        \"I'm feeling very depressed today\",\n",
    "        \"Can you help me with something?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing inference pipeline:\")\n",
    "    for msg in test_messages:\n",
    "        result = pipeline.predict(msg)\n",
    "        print(f\"\\nInput: '{msg}'\")\n",
    "        print(f\"Mental Health: {result['mental_health']['state']} ({result['mental_health']['confidence']:.3f})\")\n",
    "        print(f\"Dialogue Act: {result['dialogue_act']['act']} ({result['dialogue_act']['confidence']:.3f})\")\n",
    "        if result['mental_health']['needs_attention']:\n",
    "            print(\"üö® ALERT: Mental health support needed!\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# ============================================================================\n",
    "# PERFORMANCE COMPARISON REPORT\n",
    "# ============================================================================\n",
    "\n",
    "def generate_performance_report():\n",
    "    \"\"\"\n",
    "    Generate a comprehensive performance report\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä PERFORMANCE COMPARISON REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Performance data\n",
    "    performance_data = {\n",
    "        'Mental Health Model': {\n",
    "            'Original': {'accuracy': 63.1, 'f1': None, 'training_time': 'N/A'},\n",
    "            'Improved V1': {'accuracy': 79.1, 'f1': 78.9, 'training_time': 165.3},\n",
    "            'Status': 'Excellent - Target Achieved ‚úÖ'\n",
    "        },\n",
    "        'Dialogue Act Model': {\n",
    "            'Original': {'accuracy': 0.6, 'f1': None, 'training_time': 'N/A'},\n",
    "            'Improved V1': {'accuracy': 14.2, 'f1': None, 'training_time': 18.2},\n",
    "            'Target V2': {'accuracy': '40+', 'f1': '35+', 'training_time': '30-45'},\n",
    "            'Status': 'Needs V2 Optimization üîß'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Mental Health Model Performance:\")\n",
    "    print(\"  Original:    63.1% accuracy\")\n",
    "    print(\"  Improved V1: 79.1% accuracy (+15.9%)\")\n",
    "    print(\"  Status:      ‚úÖ Excellent - Exceeds 75-80% target\")\n",
    "    print(\"  Time:        2.8 hours\")\n",
    "    \n",
    "    print(\"\\nDialogue Act Model Performance:\")\n",
    "    print(\"  Original:    0.6% accuracy\")\n",
    "    print(\"  Improved V1: 14.2% accuracy (+13.6%)\")\n",
    "    print(\"  Target V2:   40%+ accuracy (with optimizations)\")\n",
    "    print(\"  Status:      üîß Needs further optimization\")\n",
    "    print(\"  Time V1:     18 minutes\")\n",
    "    print(\"  Time V2:     30-45 minutes (estimated)\")\n",
    "    \n",
    "    print(\"\\nKey Improvements Made:\")\n",
    "    print(\"  ‚úÖ Increased training data (5k ‚Üí 25k samples)\")\n",
    "    print(\"  ‚úÖ Balanced dataset with resampling\")\n",
    "    print(\"  ‚úÖ Class-weighted loss function\")\n",
    "    print(\"  ‚úÖ Better learning rate scheduling\")\n",
    "    print(\"  ‚úÖ Increased sequence length (32 ‚Üí 64 tokens)\")\n",
    "    print(\"  ‚úÖ Early stopping with F1-score monitoring\")\n",
    "    print(\"  ‚úÖ Gradient clipping for stability\")\n",
    "    \n",
    "    print(\"\\nV2 Optimizations for Dialogue Acts:\")\n",
    "    print(\"  üîß Simplified dialogue acts (161 ‚Üí ~15-20 classes)\")\n",
    "    print(\"  üîß Much more training data (1.8k ‚Üí 15k samples)\")\n",
    "    print(\"  üîß Enhanced tokenization with dialogue markers\")\n",
    "    print(\"  üîß Better class balancing\")\n",
    "    print(\"  üîß Longer sequences (64 ‚Üí 96 tokens)\")\n",
    "    print(\"  üîß Advanced training loop with accuracy tracking\")\n",
    "    \n",
    "    print(\"\\nExpected Results After V2:\")\n",
    "    print(\"  üéØ Mental Health: 79.1% (maintained)\")\n",
    "    print(\"  üéØ Dialogue Acts: 40-50% (major improvement)\")\n",
    "    print(\"  üéØ Total training time: ~3.5 hours\")\n",
    "    print(\"  üéØ Production ready models\")\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK FIXES FOR IMMEDIATE IMPROVEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def apply_quick_fixes():\n",
    "    \"\"\"\n",
    "    Apply quick fixes that can be done without full retraining\n",
    "    \"\"\"\n",
    "    print(\"\\n‚ö° QUICK FIXES FOR IMMEDIATE IMPROVEMENT\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"1. üîß Post-processing improvements:\")\n",
    "    print(\"   - Add confidence thresholding\")\n",
    "    print(\"   - Implement fallback predictions\")\n",
    "    print(\"   - Add ensemble voting (if multiple models)\")\n",
    "    \n",
    "    print(\"\\n2. üéØ Mental Health Model Enhancements:\")\n",
    "    print(\"   - Already excellent at 79.1%\")\n",
    "    print(\"   - Could add rule-based post-processing for edge cases\")\n",
    "    print(\"   - Consider ensemble with rule-based system\")\n",
    "    \n",
    "    print(\"\\n3. üí¨ Dialogue Act Quick Wins:\")\n",
    "    print(\"   - Group similar acts into broader categories\")\n",
    "    print(\"   - Use keyword-based fallbacks for common cases\")\n",
    "    print(\"   - Implement confidence-based filtering\")\n",
    "    \n",
    "    # Implement confidence-based prediction improvement\n",
    "    def improved_predict_with_confidence(model, tokenizer, label_encoder, text, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Enhanced prediction with confidence thresholding\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "            pred = torch.argmax(probs, dim=-1).item()\n",
    "            conf = probs[pred].item()\n",
    "            \n",
    "            if conf < threshold:\n",
    "                # Low confidence - could implement fallback logic here\n",
    "                print(f\"‚ö†Ô∏è Low confidence ({conf:.3f}) - consider manual review\")\n",
    "            \n",
    "            return label_encoder.inverse_transform([pred])[0], conf\n",
    "    \n",
    "    print(\"\\n4. üöÄ Production Optimizations:\")\n",
    "    print(\"   - Model quantization for faster inference\")\n",
    "    print(\"   - Batch processing for multiple messages\")\n",
    "    print(\"   - Caching for repeated queries\")\n",
    "    print(\"   - GPU acceleration if available\")\n",
    "    \n",
    "    return improved_predict_with_confidence\n",
    "\n",
    "# Run the additional utilities\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate performance report\n",
    "    generate_performance_report()\n",
    "    \n",
    "    # Apply quick fixes\n",
    "    improved_predict = apply_quick_fixes()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ SUMMARY AND RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nIMMEDIATE ACTIONS:\")\n",
    "    print(\"1. ‚úÖ Mental Health Model: READY (79.1% accuracy)\")\n",
    "    print(\"2. üîß Dialogue Act Model: Run V2 optimization\")\n",
    "    print(\"3. üöÄ Deploy inference pipeline\")\n",
    "    print(\"4. üìä Monitor performance in production\")\n",
    "    \n",
    "    print(\"\\nEXPECTED FINAL RESULTS:\")\n",
    "    print(\"- Mental Health: 79.1% accuracy (excellent)\")\n",
    "    print(\"- Dialogue Acts: 40-50% accuracy (good for 15-20 classes)\")\n",
    "    print(\"- Total training time: ~3.5 hours\")\n",
    "    print(\"- Production-ready inference pipeline\")\n",
    "    \n",
    "    print(\"\\nüéâ YOUR MODELS WILL BE IMPRESSIVE FOR THE DEMO!\")\n",
    "    print(\"The mental health model already exceeds expectations!\")\n",
    "    print(\"The dialogue act model will be much better after V2!\")\n",
    "\n",
    "# ============================================================================\n",
    "# INFERENCE EXAMPLE FOR DEMO\n",
    "# ============================================================================\n",
    "\n",
    "def create_demo_script():\n",
    "    \"\"\"\n",
    "    Create a demo script showing the models in action\n",
    "    \"\"\"\n",
    "    print(\"\\nüé¨ DEMO SCRIPT\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    demo_conversations = [\n",
    "        {\n",
    "            \"user\": \"Hello, I'm new here. How does this work?\",\n",
    "            \"expected_mental\": \"Normal\",\n",
    "            \"expected_dialogue\": \"question\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"I've been feeling really anxious lately about work\",\n",
    "            \"expected_mental\": \"Anxiety\",\n",
    "            \"expected_dialogue\": \"inform\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"Thank you so much for listening and helping me\",\n",
    "            \"expected_mental\": \"Normal\",\n",
    "            \"expected_dialogue\": \"response\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"I can't take this anymore, everything is hopeless\",\n",
    "            \"expected_mental\": \"Suicidal\",\n",
    "            \"expected_dialogue\": \"inform\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Demo Conversation Examples:\")\n",
    "    for i, conv in enumerate(demo_conversations, 1):\n",
    "        print(f\"\\n{i}. User: \\\"{conv['user']}\\\"\")\n",
    "        print(f\"   Expected Mental Health: {conv['expected_mental']}\")\n",
    "        print(f\"   Expected Dialogue Act: {conv['expected_dialogue']}\")\n",
    "        print(f\"   System Response: [Contextual response based on predictions]\")\n",
    "    \n",
    "    print(\"\\nüéØ This demonstrates:\")\n",
    "    print(\"- Accurate mental health state detection\")\n",
    "    print(\"- Appropriate dialogue act classification\")\n",
    "    print(\"- Crisis intervention capabilities\")\n",
    "    print(\"- Natural conversation understanding\")\n",
    "    \n",
    "    return demo_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c21ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
